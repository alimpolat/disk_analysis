{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Drive Survival Table Generator\n",
    "* creates a summary table of all 'serial_number' for each model\n",
    "* \n",
    "* writes a .csv (into OUTPUT_DIR) for each folder in DATA_FOLDERS \n",
    "* each unqiue drive (a given 'serial_number') will be one line\n",
    "* the output data is ready for survival analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from time import localtime, strftime\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEST = False\n",
    "DATA_DIR = \"data/\"\n",
    "#DATA_FOLDERS = [\"2014\",\"2015\",\"data_Q1_2016\",\"data_Q2_2016\",\"data_Q3_2016\",\"data_Q4_2016\"]\n",
    "DATA_FOLDERS = [\"data_Q1_2016\",\"data_Q2_2016\"]\n",
    "FEATURE_COLS = ['smart_9_raw','model','failure','serial_number']\n",
    "THE_TIME = strftime(\"%Y-%m-%d-%H-%S\", localtime())\n",
    "SUMMARY_DIR = \"summary_data/\"\n",
    "PERCENT_TOTAL_REQ = .05\n",
    "OUTPUT_DIR = \"survival_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def start_logger():\n",
    "    if not os.path.isdir(OUTPUT_DIR):\n",
    "        os.system(\"mkdir \" + OUTPUT_DIR)\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.DEBUG)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logfile = OUTPUT_DIR + '/' + THE_TIME + \".log\"\n",
    "    handler = logging.FileHandler(logfile, 'w')\n",
    "    handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "    logger.info(\"TEST\\t\\t=\\t\" + str(TEST))\n",
    "    logger.info(\"DATA_DIR\\t=\\t\" + DATA_DIR)\n",
    "    logger.info(\"DATA_FOLDERS\\t=\\t\" + str(DATA_FOLDERS))\n",
    "    logger.info(\"FEATURE_COLS\\t=\\t\" + str(FEATURE_COLS))\n",
    "    logger.info(\"THE_TIME\\t=\\t\" + THE_TIME)\n",
    "    logger.info(\"SUMMARY_DIR\\t=\\t\" + SUMMARY_DIR)\n",
    "    logger.info(\"PERCENT_TOTAL_REQ\\t=\\t\" + str(PERCENT_TOTAL_REQ))\n",
    "    logger.info(\"OUTPUT_DIR\\t=\\t\" + OUTPUT_DIR)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sift_data(data, model):\n",
    "    for ikey in data.keys():\n",
    "        if ikey not in FEATURE_COLS:\n",
    "            del data[ikey]\n",
    "    #print(data.columns)\n",
    "    data = data[data['model'] == model]\n",
    "    del data['model']\n",
    "    return data\n",
    "\n",
    "def tb_capacity(x):\n",
    "    \"\"\"\n",
    "    1 gig is np.power(2, 30) bytes, but maybe it is 10^9 bytes. Who you asking? Whatever.\n",
    "    \"\"\"\n",
    "    tb = np.power(10, 12)\n",
    "    #if not math.isnan(x):\n",
    "    return x/tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_by_serial(hd):\n",
    "    aggregations = {\n",
    "        'smart_9_raw': { # smart 9 is the disk uptime\n",
    "            'runtime_max': 'max',  \n",
    "            'runtime_min': 'min',\n",
    "            'uptime': lambda x: max(x) - min(x)  \n",
    "        },\n",
    "        #'model':{\n",
    "        #   'model_count': 'count',\n",
    "           #'model': 'mean'\n",
    "        #},\n",
    "        'failure': {\n",
    "         'n_obs' : 'count',\n",
    "         'failure': 'sum'\n",
    "        }  \n",
    "    }\n",
    "\n",
    "    survival = hd_all.groupby('serial_number').agg(aggregations).reset_index()\n",
    "    survival.columns = survival.columns.droplevel()\n",
    "    survival.rename(columns={'': 'serial_number'}, inplace=True)\n",
    "    return survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_data/data_Q1_2016.csv\n",
      "summary_data/data_Q2_2016.csv\n",
      "['HGST HMS5C4040ALE640' 'HGST HMS5C4040BLE640' 'Hitachi HDS5C3030ALA630'\n",
      " 'Hitachi HDS722020ALA330' 'ST4000DM000']\n"
     ]
    }
   ],
   "source": [
    "#logger = start_logger()\n",
    "#SUMMARY_DIR = \"summary_data/\"\n",
    "\n",
    "target_models = []\n",
    "for df in DATA_FOLDERS:\n",
    "    print(SUMMARY_DIR + df + '.csv')\n",
    "    summary_dats = pd.read_csv(SUMMARY_DIR + df + '.csv', header=0, nrows=200)\n",
    "    #print(temp.head)\n",
    "    summary_dats = summary_dats.sort_values(by=\"percent_total\", ascending=False)\n",
    "    clipped = summary_dats[summary_dats['percent_total'] >= PERCENT_TOTAL_REQ]\n",
    "    #print(clipped)\n",
    "    #target_models.append(clipped['model'])\n",
    "    [target_models.append(m) for m in clipped['model']]\n",
    "    \n",
    "print(np.unique(target_models))\n",
    "\n",
    "unique_target_models = np.unique(target_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with: HGST HMS5C4040ALE640\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Working with: HGST HMS5C4040BLE640\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Working with: Hitachi HDS5C3030ALA630\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Working with: Hitachi HDS722020ALA330\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Working with: ST4000DM000\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n"
     ]
    }
   ],
   "source": [
    "logger = start_logger()\n",
    "logger.info(\"Unique models working over: %s\" % unique_target_models)\n",
    "\n",
    "for model in unique_target_models:\n",
    "    logger.info(\" ### ### ### ### ### ### ### ### ### ### #### ### ###\")\n",
    "    logger.info(\"Working with: %s\" % model)\n",
    "    print(\"Working with: %s\" % model)\n",
    "    hd_all = pd.DataFrame()\n",
    "    for data_dir in DATA_FOLDERS:\n",
    "        hd_subset = pd.DataFrame()\n",
    "        data_path = DATA_DIR + data_dir + \"/\"\n",
    "        logger.info(\" * * *\")\n",
    "        logger.info(\"Loading and working with: %s\" % data_path)\n",
    "        print(\"Loading and working with: %s\" % data_path)\n",
    "        for data_file in os.listdir(data_path):\n",
    "            if data_file.split('.')[1] == 'csv':\n",
    "                if TEST:\n",
    "                    temp = pd.read_csv(data_path + data_file, header=0, nrows=20)\n",
    "                    temp = sift_data(temp, model)\n",
    "                    #print('t')\n",
    "                else:\n",
    "                    temp = pd.read_csv(data_path + data_file, header=0)\n",
    "                    temp = sift_data(temp, model)\n",
    "                #print('st: ', np.shape(temp))\n",
    "                #print ('tt', type(temp))\n",
    "                hd_subset = hd_subset.append(temp)\n",
    "                #print('shss: ', np.shape(hd_subset))\n",
    "        #hd.capacity_bytes = hd.capacity_bytes.map(tb_capacity)\n",
    "        #hd.rename(columns={'capacity_bytes': 'capacity_tb'}, inplace=True)\n",
    "        #hd['date'] = hd['date'].apply(pd.to_datetime)\n",
    "        #print(hd_subset.head(5))\n",
    "        logger.info(\"There are %d unique drives in the subset. \" % hd_subset['serial_number'].value_counts().count())\n",
    "        #logger.info(\"There are %d unique models in the subset. \" % hd_subset['model'].value_counts().count())\n",
    "        logger.info(\"There are %d failures in the subset.\" % hd_subset['failure'].sum())\n",
    "        hd_all = hd_all.append(hd_subset)\n",
    "    logger.info(\"There are %d unique drives in the complete set. \" % hd_all['serial_number'].value_counts().count())\n",
    "    #logger.info(\"There are %d unique models in the complete set. \" % hd_all['model'].value_counts().count())\n",
    "    logger.info(\"There are %d failures in the complete set.\" % hd_all['failure'].sum())\n",
    "\n",
    "    survives = aggregate_by_serial(hd_all)\n",
    "    #print(survives.head(5))\n",
    "    \n",
    "    #x = 'alpha beta gamma'\n",
    "    model_ns = model.replace(\" \", \"_\")\n",
    "    survives.to_csv(OUTPUT_DIR + \"/survival_\" + model_ns + \".csv\", index = False)\n",
    "    #survives.to_csv(OUTPUT_DIR + \"/survival_\" + model_ns + data_dir + \".csv\", index = False)\n",
    "        \n",
    "        \n",
    "#summary = summarize(hd)\n",
    "#summary.to_csv(OUTPUT_DIR + \"/\" + THE_TIME + data_dir + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
