{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Drive Survival Table Generator\n",
    "* creates a summary table of all 'serial_number' for each model\n",
    "* this program process a lot of data and takes a long time!\n",
    "* it process the data one model at a time to save memory\n",
    "* writes a .csv (into OUTPUT_DIR) for each model, each unqiue drive (a given 'serial_number') will be one line \n",
    "* the output data is ready for survival analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from time import localtime, strftime, time\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEST = False\n",
    "DATA_DIR = \"data/\"\n",
    "DATA_FOLDERS = [\"2014\",\"2015\",\"data_Q1_2016\",\"data_Q2_2016\",\"data_Q3_2016\",\"data_Q4_2016\"]\n",
    "FEATURE_COLS = ['smart_9_raw','model','failure','serial_number']\n",
    "THE_TIME = strftime(\"%Y-%m-%d-%H-%M\", localtime())\n",
    "SUMMARY_DIR = \"summary_data/\"\n",
    "PERCENT_TOTAL_REQ = 0.0\n",
    "FAILURE_RATE_REQ = 0\n",
    "MIN_NUMBER_REQ = 100\n",
    "OUTPUT_DIR = \"survival_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def start_logger():\n",
    "    if not os.path.isdir(OUTPUT_DIR):\n",
    "        os.system(\"mkdir \" + OUTPUT_DIR)\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.DEBUG)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logfile = OUTPUT_DIR + '/' + THE_TIME + \".log\"\n",
    "    handler = logging.FileHandler(logfile, 'w')\n",
    "    handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "    logger.info(\"TEST\\t\\t=\\t\" + str(TEST))\n",
    "    logger.info(\"DATA_DIR\\t=\\t\" + DATA_DIR)\n",
    "    logger.info(\"DATA_FOLDERS\\t=\\t\" + str(DATA_FOLDERS))\n",
    "    logger.info(\"FEATURE_COLS\\t=\\t\" + str(FEATURE_COLS))\n",
    "    logger.info(\"THE_TIME\\t=\\t\" + THE_TIME)\n",
    "    logger.info(\"SUMMARY_DIR\\t=\\t\" + SUMMARY_DIR)\n",
    "    logger.info(\"PERCENT_TOTAL_REQ\\t=\\t\" + str(PERCENT_TOTAL_REQ))\n",
    "    logger.info(\"FAILURE_RATE_REQ\\t=\\t\" + str(FAILURE_RATE_REQ))\n",
    "    logger.info(\"MIN_NUMBER_REQ\\t=\\t\" + str(MIN_NUMBER_REQ))\n",
    "    logger.info(\"OUTPUT_DIR\\t=\\t\" + OUTPUT_DIR)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sift_data(data, model):\n",
    "    for ikey in data.keys():\n",
    "        if ikey not in FEATURE_COLS:\n",
    "            del data[ikey]\n",
    "    #print(data.columns)\n",
    "    data = data[data['model'] == model]\n",
    "    del data['model']\n",
    "    return data\n",
    "\n",
    "def tb_capacity(x):\n",
    "    \"\"\"\n",
    "    1 gig is np.power(2, 30) bytes, but maybe it is 10^9 bytes. Who you asking? Whatever.\n",
    "    \"\"\"\n",
    "    tb = np.power(10, 12)\n",
    "    #if not math.isnan(x):\n",
    "    return x/tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_by_serial(hd):\n",
    "    aggregations = {\n",
    "        'smart_9_raw': { # smart 9 is the disk uptime\n",
    "            'runtime_max': 'max',  \n",
    "            'runtime_min': 'min',\n",
    "            'uptime': lambda x: max(x) - min(x)  \n",
    "        },\n",
    "        #'model':{\n",
    "        #   'model_count': 'count',\n",
    "           #'model': 'mean'\n",
    "        #},\n",
    "        'failure': {\n",
    "         'n_obs' : 'count',\n",
    "         'failure': 'sum'\n",
    "        }  \n",
    "    }\n",
    "\n",
    "    survival = hd_all.groupby('serial_number').agg(aggregations).reset_index()\n",
    "    survival.columns = survival.columns.droplevel()\n",
    "    survival.rename(columns={'': 'serial_number'}, inplace=True)\n",
    "    return survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_data/2014.csv\n",
      "summary_data/2015.csv\n",
      "summary_data/data_Q1_2016.csv\n",
      "summary_data/data_Q2_2016.csv\n",
      "summary_data/data_Q3_2016.csv\n",
      "summary_data/data_Q4_2016.csv\n",
      "['HGST HDS5C4040ALE630' 'HGST HDS724040ALE640' 'HGST HMS5C4040ALE640'\n",
      " 'HGST HMS5C4040BLE640' 'HGST HUH728080ALE600' 'Hitachi HDS5C3030ALA630'\n",
      " 'Hitachi HDS5C3030BLE630' 'Hitachi HDS5C4040ALE630'\n",
      " 'Hitachi HDS722020ALA330' 'Hitachi HDS723020BLA642'\n",
      " 'Hitachi HDS723030ALA640' 'Hitachi HDS723030BLE640'\n",
      " 'Hitachi HDS724040ALE640' 'Hitachi HDT721010SLA360'\n",
      " 'Hitachi HDT725025VLA380' 'SAMSUNG HD103UJ' 'SAMSUNG HD154UI'\n",
      " 'ST1000LM024 HN' 'ST1500DL001' 'ST1500DL003' 'ST1500DM003' 'ST2000DL001'\n",
      " 'ST2000DL003' 'ST2000DM001' 'ST2000VN000' 'ST250LM004 HN' 'ST250LT007'\n",
      " 'ST3000DM001' 'ST31500341AS' 'ST31500541AS' 'ST3160316AS' 'ST3160318AS'\n",
      " 'ST32000542AS' 'ST320005XXXX' 'ST320LT007' 'ST33000651AS' 'ST3500320AS'\n",
      " 'ST4000DM000' 'ST4000DX000' 'ST4000DX002' 'ST500LM012 HN' 'ST6000DM001'\n",
      " 'ST6000DX000' 'ST8000DM002' 'ST8000NM0055' 'ST9250315AS' 'ST9320325AS'\n",
      " 'TOSHIBA DT01ACA300' 'TOSHIBA MD04ABA400V' 'TOSHIBA MD04ABA500V'\n",
      " 'TOSHIBA MQ01ABF050' 'WDC WD1000FYPS' 'WDC WD1001FALS' 'WDC WD10EACS'\n",
      " 'WDC WD10EADS' 'WDC WD10EADX' 'WDC WD10EALS' 'WDC WD10EARS' 'WDC WD10EARX'\n",
      " 'WDC WD15EARS' 'WDC WD1600AAJB' 'WDC WD1600AAJS' 'WDC WD1600BPVT'\n",
      " 'WDC WD20EFRX' 'WDC WD2500AAJB' 'WDC WD2500AAJS' 'WDC WD2500BEVT'\n",
      " 'WDC WD2500BPVT' 'WDC WD2500JB' 'WDC WD30EFRX' 'WDC WD30EZRS'\n",
      " 'WDC WD30EZRX' 'WDC WD3200AAJB' 'WDC WD3200AAJS' 'WDC WD3200AAKS'\n",
      " 'WDC WD3200BEKT' 'WDC WD3200BEKX' 'WDC WD3200LPVX' 'WDC WD40EFRX'\n",
      " 'WDC WD5000AAJS' 'WDC WD5000BPKT' 'WDC WD5000LPCX' 'WDC WD5000LPVX'\n",
      " 'WDC WD5002ABYS' 'WDC WD5003ABYX' 'WDC WD60EFRX' 'WDC WD800AAJB'\n",
      " 'WDC WD800AAJS' 'WDC WD800BB' 'WDC WD800JB' 'WDC WD800JD' 'WDC WD800LB']\n"
     ]
    }
   ],
   "source": [
    "#logger = start_logger()\n",
    "#SUMMARY_DIR = \"summary_data/\"\n",
    "\n",
    "target_models = []\n",
    "for df in DATA_FOLDERS:\n",
    "    print(SUMMARY_DIR + df + '.csv')\n",
    "    summary_dats = pd.read_csv(SUMMARY_DIR + df + '.csv', header=0, nrows=200)\n",
    "    #print(temp.head)\n",
    "    summary_dats = summary_dats.sort_values(by=\"percent_total\", ascending=False)\n",
    "    clipped1 = summary_dats[summary_dats['percent_total'] >= PERCENT_TOTAL_REQ]\n",
    "    clipped2 = summary_dats[(summary_dats['failure_rate'] >= FAILURE_RATE_REQ) & (summary_dats['drive_count'] >= MIN_NUMBER_REQ)]\n",
    "    [target_models.append(m) for m in clipped1['model']]\n",
    "    [target_models.append(m) for m in clipped2['model']]\n",
    "\n",
    "    \n",
    "print(np.unique(target_models))\n",
    "\n",
    "unique_target_models = np.unique(target_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with: HGST HMS5C4040ALE640\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: HGST HMS5C4040BLE640\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: Hitachi HDS5C3030ALA630\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: Hitachi HDS5C4040ALE630\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: Hitachi HDS722020ALA330\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: ST3000DM001\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: ST31500341AS\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: ST31500541AS\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: ST32000542AS\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: ST4000DM000\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: ST4000DX000\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: ST500LM012 HN\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: ST8000DM002\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: WDC WD10EACS\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: WDC WD10EADS\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: WDC WD1600AAJS\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: WDC WD20EFRX\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: WDC WD30EFRX\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: WDC WD30EZRX\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: WDC WD5000LPVX\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n",
      "Working with: WDC WD60EFRX\n",
      "Loading and working with: data/2014/\n",
      "Loading and working with: data/2015/\n",
      "Loading and working with: data/data_Q1_2016/\n",
      "Loading and working with: data/data_Q2_2016/\n",
      "Loading and working with: data/data_Q3_2016/\n",
      "Loading and working with: data/data_Q4_2016/\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "logger = start_logger()\n",
    "logger.info(\"Unique models working over: %s\" % unique_target_models)\n",
    "\n",
    "for model in unique_target_models:\n",
    "    logger.info(\" ### ### ### ### ### ### ### ### ### ### #### ### ###\")\n",
    "    logger.info(\"Working with: %s\" % model)\n",
    "    logger.info(\"Elapsed time: %s seconds\" % np.round(time() - start_time,1))\n",
    "    print(\"Working with: %s\" % model)\n",
    "    hd_all = pd.DataFrame()\n",
    "    for data_dir in DATA_FOLDERS:\n",
    "        hd_subset = pd.DataFrame()\n",
    "        data_path = DATA_DIR + data_dir + \"/\"\n",
    "        logger.info(\" * * *\")\n",
    "        logger.info(\"Loading and working with: %s\" % data_path)\n",
    "        print(\"Loading and working with: %s\" % data_path)\n",
    "        for data_file in os.listdir(data_path):\n",
    "            if data_file.split('.')[1] == 'csv':\n",
    "                if TEST:\n",
    "                    temp = pd.read_csv(data_path + data_file, header=0, nrows=20)\n",
    "                    temp = sift_data(temp, model)\n",
    "                    #print('t')\n",
    "                else:\n",
    "                    temp = pd.read_csv(data_path + data_file, header=0)\n",
    "                    temp = sift_data(temp, model)\n",
    "                #print('st: ', np.shape(temp))\n",
    "                #print ('tt', type(temp))\n",
    "                hd_subset = hd_subset.append(temp)\n",
    "                #print('shss: ', np.shape(hd_subset))\n",
    "        #hd.capacity_bytes = hd.capacity_bytes.map(tb_capacity)\n",
    "        #hd.rename(columns={'capacity_bytes': 'capacity_tb'}, inplace=True)\n",
    "        #hd['date'] = hd['date'].apply(pd.to_datetime)\n",
    "        #print(hd_subset.head(5))\n",
    "        logger.info(\"There are %d unique drives in the subset. \" % hd_subset['serial_number'].value_counts().count())\n",
    "        #logger.info(\"There are %d unique models in the subset. \" % hd_subset['model'].value_counts().count())\n",
    "        logger.info(\"There are %d failures in the subset.\" % hd_subset['failure'].sum())\n",
    "        hd_all = hd_all.append(hd_subset)\n",
    "    logger.info(\"There are %d unique drives in the complete set. \" % hd_all['serial_number'].value_counts().count())\n",
    "    #logger.info(\"There are %d unique models in the complete set. \" % hd_all['model'].value_counts().count())\n",
    "    logger.info(\"There are %d failures in the complete set.\" % hd_all['failure'].sum())\n",
    "\n",
    "    survives = aggregate_by_serial(hd_all)\n",
    "    #print(survives.head(5))\n",
    "    \n",
    "    #x = 'alpha beta gamma'\n",
    "    model_ns = model.replace(\" \", \"_\")\n",
    "    survives.to_csv(OUTPUT_DIR + \"/survival_\" + model_ns + \".csv\", index = False)\n",
    "    #survives.to_csv(OUTPUT_DIR + \"/survival_\" + model_ns + data_dir + \".csv\", index = False)\n",
    "        \n",
    "        \n",
    "#summary = summarize(hd)\n",
    "#summary.to_csv(OUTPUT_DIR + \"/\" + THE_TIME + data_dir + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
