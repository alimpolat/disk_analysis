{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SMART features aren't always very predictive to hard drive failure, quantify that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import struct\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "from bokeh.models import Range1d\n",
    "from bokeh.charts import Bar, output_file, show\n",
    "from bokeh.models import ColumnDataSource, Plot, Range1d\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.plotting import figure\n",
    "import pickle\n",
    "output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FEATURE_COLS = [u'smart_1_raw', u'smart_3_raw', u'smart_4_raw', \n",
    "        u'smart_5_raw', u'smart_7_raw', u'smart_9_raw', u'smart_10_raw',\n",
    "        u'smart_12_raw', u'smart_183_raw', u'smart_184_raw', u'smart_187_raw', \n",
    "        u'smart_188_raw', u'smart_189_raw', u'smart_190_raw', u'smart_191_raw', \n",
    "        u'smart_192_raw', u'smart_193_raw', u'smart_194_raw', u'smart_197_raw',\n",
    "        u'smart_198_raw', u'smart_199_raw', u'smart_240_raw', u'smart_241_raw',\n",
    "        u'smart_242_raw', u'NaNs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the disk_bin.pkl file (created by HD_Extract_Features) is a list of pandas data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pkl_file = open('data/disk_bin.pkl', 'rb')\n",
    "disk_bin = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "print len(disk_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### searching for hyperparams\n",
    "\n",
    "What error should we minimize?\n",
    "\n",
    "f1 = 2 * ( precsion * recall )  / (precsion + recall)\n",
    "\n",
    "p = tp / (fp + tp)\n",
    "\n",
    "r = tp / (fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_error(estimator, X, y):\n",
    "    cv = model_selection.ShuffleSplit(n_splits=2, test_size=0.3, random_state=np.random.randint(1,1000))\n",
    "    cv_score = model_selection.cross_val_score(estimator, X, y, cv=cv, scoring='f1')\n",
    "    print 'cross validated f1 scores: ', cv_score\n",
    "    return cv_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_important(estimator, x,y):\n",
    "    \"\"\"\n",
    "    Use estimator find important features\n",
    "    Get the indices and sort the labels of the important features\n",
    "    \"\"\"\n",
    "    estimator.fit(x, y)\n",
    "    importance = estimator.feature_importances_\n",
    "    std_err = np.std([tree.feature_importances_ for tree in estimator.estimators_], axis=0)\n",
    "    indices = np.argsort(importance)[::-1]\n",
    "    return importance, std_err, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def best_hyperparams(pipe, x, y, hyper1_range, hyper2_range, params, ptitle='junk.png'): \n",
    "    x_mesh_range = hyper1_range\n",
    "    y_mesh_range = hyper2_range\n",
    "\n",
    "    x_mesh, y_mesh = np.meshgrid(x_mesh_range, y_mesh_range)\n",
    "    in_sample = np.zeros([len(y_mesh_range),len(x_mesh_range)])\n",
    "    cv = np.zeros([len(y_mesh_range),len(x_mesh_range)])\n",
    "\n",
    "    for i in range(len(y_mesh_range)):\n",
    "        for j in range(len(x_mesh_range)):\n",
    "            params[params.keys()[0]] = x_mesh_range[j]\n",
    "            params[params.keys()[1]] = y_mesh_range[i]\n",
    "            print params\n",
    "            #print **params\n",
    "            pipe.set_params(**params)\n",
    "            #pipe.set_params(n_estimators=y_mesh_range[i])\n",
    "            pipe.fit(x,y)\n",
    "            y_pred = pipe.predict(x)\n",
    "            #in_sample_error_ = metrics.accuracy_score(y, y_pred)\n",
    "            in_sample_error_ = metrics.f1_score(y, y_pred)\n",
    "            out_of_sample_error_ = compute_error(pipe, x, y)\n",
    "            print 'prediction at y =',  y_mesh_range[i], 'prediction at x =', x_mesh_range[j]\n",
    "            print in_sample_error_ ,  out_of_sample_error_\n",
    "            in_sample[i][j] = in_sample_error_\n",
    "            cv[i][j] = out_of_sample_error_\n",
    "        \n",
    "    fig = plt.figure(figsize=(10.5, 9))\n",
    "    cm1 = plt.cm.get_cmap('cubehelix')\n",
    "    plt.contourf(x_mesh, y_mesh, cv, alpha=0.9, cmap=cm1)\n",
    "    cbar = plt.colorbar()\n",
    "    #cbar.ax.set_ylabel('Accuracy')\n",
    "    plt.title('Cross Validated f1')\n",
    "    plt.xlabel(params.keys()[0])\n",
    "    plt.ylabel(params.keys()[1])\n",
    "    #plt.show()\n",
    "    fig.savefig(\"plots/cv_\" + ptitle)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10.5, 9))\n",
    "    cm1 = plt.cm.get_cmap('cubehelix')\n",
    "    plt.contourf(x_mesh, y_mesh, in_sample, alpha=0.9, cmap=cm1)\n",
    "    cbar = plt.colorbar()\n",
    "    #cbar.ax.set_ylabel('Accuracy')\n",
    "    plt.title('Sample f1')\n",
    "    plt.xlabel(params.keys()[0])\n",
    "    plt.ylabel(params.keys()[1])\n",
    "    #plt.show()\n",
    "    fig.savefig(\"plots/s_\" + ptitle)\n",
    "    \n",
    "    best_accuracy = cv[np.unravel_index(np.argmax(cv), cv.shape)]\n",
    "    best_x =  x_mesh[np.unravel_index(np.argmax(cv), cv.shape)]\n",
    "    best_y = y_mesh[np.unravel_index(np.argmax(cv), cv.shape)]\n",
    "    print best_x, best_y\n",
    "    return best_x, best_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def super_fit_with_hyper_friends_erf(x,y):\n",
    "    all_feats= FEATURE_COLS\n",
    "    erf = ExtraTreesClassifier()\n",
    "    #min_samples_split \n",
    "    hyper1_range = np.arange(2, 16, 3)\n",
    "    #min_samples_leaf\n",
    "    hyper2_range = np.arange(2, 24, 3)\n",
    "    params = {'min_samples_split':666, 'min_samples_leaf':666}\n",
    "    ptitle = 'erf_msml_r1_day' + str(nday) + '.png'\n",
    "    min_samples_split, min_samples_leaf = best_hyperparams(erf, x, y, \n",
    "                                                               hyper1_range, \n",
    "                                                               hyper2_range, \n",
    "                                                               params, \n",
    "                                                               ptitle=ptitle)\n",
    "    ###\n",
    "                             \n",
    "    erf = ExtraTreesClassifier(min_samples_split=min_samples_split, \n",
    "                               min_samples_leaf=min_samples_leaf)\n",
    "    hyper1_range = np.arange(20, len(all_feats), 1)\n",
    "    hyper2_range = np.arange(128, 212, 32)\n",
    "    params = {'max_features':666, 'n_estimators':666}\n",
    "    ptitle = 'erf_mfne_r1_day' + str(nday) + '.png'\n",
    "    max_features, n_estimators = best_hyperparams(erf, x, y, \n",
    "                                                  hyper1_range, \n",
    "                                                  hyper2_range,\n",
    "                                                  params,\n",
    "                                                  ptitle=ptitle)\n",
    "    \n",
    "    erf = ExtraTreesClassifier(n_estimators=n_estimators, \n",
    "                               min_samples_split=min_samples_split,\n",
    "                               min_samples_leaf=min_samples_leaf, \n",
    "                               max_features=max_features)\n",
    "    \n",
    "    ###lass_weight  [\"balanced\", \"balanced_subsample\", \"None\"]\n",
    "    \n",
    "    hyper1_range = [1e-8,1e-5,1e-4]\n",
    "    hyper2_range = [0, 0.0001, .0005, .001]\n",
    "    params = {'min_impurity_split':666, 'min_weight_fraction_leaf':666}\n",
    "    ptitle = 'erf_mismwf_r1_day' + str(nday) + '.png'\n",
    "    min_impurity_split, min_weight_fraction_leaf = best_hyperparams(erf, x, y, \n",
    "                                                  hyper1_range, \n",
    "                                                  hyper2_range,\n",
    "                                                  params,\n",
    "                                                  ptitle=ptitle)\n",
    "    \n",
    "    \n",
    "    erf = ExtraTreesClassifier(n_estimators=n_estimators, \n",
    "                               min_samples_split=min_samples_split,\n",
    "                               min_samples_leaf=min_samples_leaf, \n",
    "                               max_features=max_features,\n",
    "                               min_impurity_split = min_impurity_split,\n",
    "                               min_weight_fraction_leaf= min_weight_fraction_leaf)\n",
    "    \n",
    "    importance, std_err, indices = get_important(erf, x, y)\n",
    "    \n",
    "    out_f_name = \"data/\" + DIR_PATH + \"/importance\" + str(nday) + \".txt\"\n",
    "    out_file = open(out_f_name, \"w\")\n",
    "    for f in range(x.shape[1]):\n",
    "        cname = x.columns[indices[f]]\n",
    "        impor = importance[indices[f]]\n",
    "        s_err = std_err[indices[f]]\n",
    "        print(\"Rank %d: %s (%f) (%f)\" % (f + 1, cname, impor, s_err))\n",
    "        tmp = str(cname) + ',' + str(impor) + ',' + str(s_err) + \"\\n\"\n",
    "        out_file.write(tmp)\n",
    "    out_file.close()\n",
    "    \n",
    "    importance_array = zip(x.columns, importance, std_err)\n",
    "    return importance_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The incoming list of data frames in disk_bin is ragged: not all disks have N days of observations. The vast majority of incoming disks have a set N number of contiguous days and all should have [28] features (N days x [28] features). The disks will get put into the analysis if their number of days >= LOOKBACK_DAYS, so for longer LOOKBACK_DAYS there will be less disks, especially faiued disks :(\n",
    "\n",
    "gridcv was avoidoid simply because I wanted to use a custom fitter that would make contour plots of the fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LOOKBACK_DAYS = [60,50,40,30,20,15,10,9,8,7,6,5,4,3,2,1,0]\n",
    "LOOKBACK_DAYS = [0, 1, 2, 3, 4]\n",
    "DIR_PATH = \"ST4000DM000_data\"\n",
    "TEST = False\n",
    "\n",
    "all_days_feat_imps = [[c] for c in FEATURE_COLS]\n",
    "all_days_feat_stds = [[c] for c in FEATURE_COLS]\n",
    "\n",
    "for nday in LOOKBACK_DAYS:\n",
    "    print \"Lookback time: \", nday\n",
    "    n = 0\n",
    "    x = pd.DataFrame()\n",
    "    for disk in disk_bin:\n",
    "        if len(disk) - nday - 1 >= 0: \n",
    "            x = x.append(disk.loc[len(disk) - nday - 1])\n",
    "        n += 1\n",
    "        if TEST and n > 500:\n",
    "            break\n",
    "    print len(x)\n",
    "    x = x.reset_index(drop=True)\n",
    "    y = x['failure']\n",
    "    for col in x.columns:\n",
    "        if col not in FEATURE_COLS:\n",
    "            del x[col]        \n",
    "            \n",
    "    importance_array = super_fit_with_hyper_friends_erf(x,y)\n",
    "    for i in range(len(FEATURE_COLS)):\n",
    "        for im in importance_array:\n",
    "            if im[0] == FEATURE_COLS[i]:\n",
    "                all_days_feat_stds[i].append(im[2])\n",
    "                all_days_feat_imps[i].append(im[1])\n",
    "                \n",
    "header = 'col'\n",
    "for d in LOOKBACK_DAYS:\n",
    "    header += \",\" +str(d) \n",
    "np.savetxt(\"data/\" + DIR_PATH + \"/all_days_feat_imps.txt\", \n",
    "           all_days_feat_imps, delimiter=',', fmt='%s', newline='\\n', header=header, comments ='')\n",
    "np.savetxt(\"data/\" + DIR_PATH + \"/all_days_feat_stds.txt\", \n",
    "           all_days_feat_stds, delimiter=',', fmt='%s', newline='\\n', header=header, comments ='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Make plot of feature importance for single day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nday = 0\n",
    "f_name = \"data/\" + DIR_PATH + \"/importance\" + str(nday) + \".txt\"\n",
    "importance_df = pd.read_csv(f_name, names = [\"feature\",\"importance\",\"std_err\"])\n",
    "\n",
    "title = \"Feature Importance (Lookback Day=\" + str(nday) + \")\"\n",
    "\n",
    "imedian = np.median(importance_df['importance'])\n",
    "\n",
    "cimportance_df = importance_df[importance_df['importance'] >= .01]\n",
    "\n",
    "plot = figure(title=title, width=700, height=600, y_range=cimportance_df.loc[:,\"feature\"].values.tolist()) \n",
    "\n",
    "\n",
    "plot.line([imedian, imedian], [0.1, len(cimportance_df)+1.1], line_width=2, color=(180,200,200))\n",
    "for idx in cimportance_df.index:\n",
    "    col_width = cimportance_df.loc[idx,'importance']\n",
    "    err_size = cimportance_df.loc[idx,'std_err']\n",
    "    c1, c2, c3 = 100+idx*2, 110+6*idx,160+4*idx\n",
    "    plot.hbar(y=idx+1.0, height=0.8, left=0, right=col_width, color=(c1,c2,c3))\n",
    "    plot.line([col_width-err_size/2., col_width+err_size/2.], [idx+1.0, idx+1.0], line_width=2, color=(0,0,0))\n",
    "\n",
    "plot.xaxis.axis_label = 'Importance'\n",
    "plot.yaxis.axis_label = 'Feature'\n",
    "plot.x_range = Range1d(-.005, 1.00)\n",
    "plot.grid.grid_line_alpha = 0\n",
    "plot.ygrid.grid_line_color = None\n",
    "plot.toolbar.logo = None\n",
    "plot.outline_line_width = 0\n",
    "plot.outline_line_color = \"white\"\n",
    "plot.yaxis.major_tick_line_width = 2\n",
    "plot.xaxis.major_tick_line_width = 2\n",
    "plot.xaxis.axis_line_width = 2\n",
    "plot.yaxis.axis_line_width = 2\n",
    "plot.title.text_font_size = '16pt'\n",
    "plot.xaxis.axis_label_text_font_size = \"14pt\"\n",
    "plot.xaxis.major_label_text_font_size = \"14pt\"\n",
    "plot.yaxis.axis_label_text_font_size = \"14pt\"\n",
    "plot.yaxis.major_label_text_font_size = \"14pt\"\n",
    "    \n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make plot of feature importance for many days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = \"Feature Importance\"\n",
    "fname = \"data/\" + DIR_PATH + \"/all_days_feat_imps.txt\"\n",
    "importance_df = pd.read_csv(fname)\n",
    "plot = figure(title=title, width=700, height=900, y_range=list(importance_df['col'])) \n",
    "days = importance_df.columns.values[1:len(importance_df.columns.values)]\n",
    "len_days = len(days)\n",
    "h = 1.0/len_days  - .09\n",
    "n = 0\n",
    "for n in importance_df.index:\n",
    "    j = 0.\n",
    "    for d in days:\n",
    "        importance = importance_df[d].loc[n]\n",
    "        c1, c2, c3 = 100+n*2, 110+5*n,160+3*n\n",
    "        c1, c2, c3 = 60+j*3, 90+22*j,130+18*j\n",
    "        plot.hbar(y=1 + n - .4 + .9*j/len_days, height=h, left=0, right=importance, color=(c1,c2,c3))\n",
    "        #plot.line([col_width-err_size/2., col_width+err_size/2.], [idx+1.1, idx+1.1], line_width=2, color=(0,0,0))\n",
    "        j += 1.0\n",
    "        \n",
    "plot.xaxis.axis_label = 'Importance'\n",
    "plot.yaxis.axis_label = 'Feature'\n",
    "plot.x_range = Range1d(-.005, 0.3)\n",
    "plot.grid.grid_line_alpha = 0\n",
    "plot.ygrid.grid_line_color = None\n",
    "plot.toolbar.logo = None\n",
    "plot.outline_line_width = 0\n",
    "plot.outline_line_color = \"white\"\n",
    "plot.yaxis.major_tick_line_width = 2\n",
    "plot.xaxis.major_tick_line_width = 2\n",
    "plot.xaxis.axis_line_width = 2\n",
    "plot.yaxis.axis_line_width = 2\n",
    "plot.title.text_font_size = '16pt'\n",
    "plot.xaxis.axis_label_text_font_size = \"14pt\"\n",
    "plot.xaxis.major_label_text_font_size = \"14pt\"\n",
    "plot.yaxis.axis_label_text_font_size = \"14pt\"\n",
    "plot.yaxis.major_label_text_font_size = \"14pt\"\n",
    "    \n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make plot of feature importance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title = \"Feature Importance Over Lookback Time\"\n",
    "#base_df = importance_bin[4]\n",
    "\n",
    "\n",
    "importance_df = pd.read_csv(fname)\n",
    "\n",
    "plot = figure(title=title, width=800, height=700)\n",
    "    \n",
    "ct =        [\"#8c564b\", #brown\n",
    "                \"#1f77b4\", #blue           0\n",
    "                \"#17becf\", #blue2          2\n",
    "                \"#9edae5\", #light_blue2   3\n",
    "                \"#98df8a\", #light_green       4\n",
    "                \"#d62728\", #red             5\n",
    "                \"#ff7f0e\", #oragne          6\n",
    "                \"#ffbb78\", #peach       7\n",
    "                \"#aec7e8\", #light_blue    1\n",
    "                \"#bcbd22\", #yellow         8\n",
    "                \"#dbdb8d\", #beige           9\n",
    "                \"#9467bd\", #Purples       10\n",
    "                \"#ff9896\", #pink            \n",
    "                \"#e377c2\", #soft_pink    \n",
    "                \"#f7b6d2\", #very_soft_pink     13       \n",
    "                \"#c5b0d5\", #lavender\n",
    "                \"#2ca02c\", #green\n",
    "                \"#c49c94\", #bland           \n",
    "                \"#7f7f7f\", #dark_grey\n",
    "                \"#c7c7c7\"] #grey\n",
    "    \n",
    "days = list(importance_df.columns.values[1:len(importance_df.columns.values)])\n",
    "\n",
    "len_days = len(days)\n",
    "\n",
    "h = 1.0/len_days  - .09\n",
    "j = 0\n",
    "for n in importance_df.index:\n",
    "    row =  list(importance_df.loc[n])\n",
    "    smart = row[0]\n",
    "    importances = row[1::]\n",
    "    if np.median(importances) <= .025:\n",
    "        continue\n",
    "    source = ColumnDataSource(\n",
    "        data=dict(\n",
    "            timeline=days,\n",
    "            important=[i for i in importances]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    tmp = plot.line('timeline', 'important', line_width = 2, alpha=.8, \n",
    "                    source = source,color=ct[j], legend = smart)\n",
    "    j += 1\n",
    "        \n",
    "plot.yaxis.axis_label = 'Importance'\n",
    "plot.xaxis.axis_label = 'Lookback Day'\n",
    "plot.x_range = Range1d(-.005, 50)\n",
    "plot.grid.grid_line_alpha = 0\n",
    "plot.ygrid.grid_line_color = None\n",
    "plot.toolbar.logo = None\n",
    "plot.outline_line_width = 0\n",
    "plot.outline_line_color = \"white\"\n",
    "plot.yaxis.major_tick_line_width = 2\n",
    "plot.xaxis.major_tick_line_width = 2\n",
    "plot.xaxis.axis_line_width = 2\n",
    "plot.yaxis.axis_line_width = 2\n",
    "plot.title.text_font_size = '16pt'\n",
    "plot.xaxis.axis_label_text_font_size = \"14pt\"\n",
    "plot.xaxis.major_label_text_font_size = \"14pt\"\n",
    "plot.yaxis.axis_label_text_font_size = \"14pt\"\n",
    "plot.yaxis.major_label_text_font_size = \"14pt\"\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
