{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Drive Model Summary Generator\n",
    "* makes a summary of hard drive models for each data period\n",
    "* writes a .csv (into OUTPUT_DIR) for each folder in DATA_FOLDERS \n",
    "* the output data summary columns might include:\n",
    " ['model', 'size', 'count', 'days', 'obs_days', 'runtime', 'obs_runtime','failure_rate', 'obs_failure_rate', 'percent_total', 'failures']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from time import localtime, strftime\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEST = False\n",
    "DATA_DIR = \"data/\"\n",
    "DATA_FOLDERS = [\"2014\",\"2015\",\"data_Q1_2016\",\"data_Q2_2016\",\"data_Q3_2016\",\"data_Q4_2016\"]\n",
    "#DATA_FOLDERS = [\"data_Q1_2016\",\"data_Q2_2016\"]\n",
    "FEATURE_COLS = ['date','capacity_bytes','smart_9_raw','model','failure','serial_number']\n",
    "THE_TIME = strftime(\"%Y-%m-%d-%H-%S\", localtime())\n",
    "OUTPUT_DIR = \"summary_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def start_logger():\n",
    "    if not os.path.isdir(OUTPUT_DIR):\n",
    "        os.system(\"mkdir \" + OUTPUT_DIR)\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.DEBUG)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logfile = OUTPUT_DIR + '/' + THE_TIME + \".log\"\n",
    "    handler = logging.FileHandler(logfile, 'w')\n",
    "    handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "    logger.info(\"TEST\\t\\t=\\t\" + str(TEST))\n",
    "    logger.info(\"DATA_DIR\\t=\\t\" + DATA_DIR)\n",
    "    logger.info(\"DATA_FOLDERS\\t=\\t\" + str(DATA_FOLDERS))\n",
    "    logger.info(\"FEATURE_COLS\\t=\\t\" + str(FEATURE_COLS))\n",
    "    logger.info(\"THE_TIME\\t=\\t\" + THE_TIME)\n",
    "    logger.info(\"OUTPUT_DIR\\t=\\t\" + OUTPUT_DIR)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sift_data(data):\n",
    "    for ikey in data.keys():\n",
    "        if ikey not in FEATURE_COLS:\n",
    "            del data[ikey]\n",
    "    return data\n",
    "\n",
    "\n",
    "def tb_capacity(x):\n",
    "    \"\"\"\n",
    "    1 gig is np.power(2, 30) bytes, but maybe it is 10^9 bytes. Who you asking? Whatever.\n",
    "    \"\"\"\n",
    "    tb = np.power(10, 12)\n",
    "    #if not math.isnan(x):\n",
    "    return x/tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def summarize(data):\n",
    "    aggregations = {\n",
    "    'failure': {\n",
    "     'failure': 'sum'\n",
    "    },\n",
    "    'capacity_tb':{\n",
    "        'mean_cap': 'mean'\n",
    "    },\n",
    "    'date': { \n",
    "        #'max_days': 'max'\n",
    "        #'min_days': 'min'\n",
    "        'days': 'count',\n",
    "        'obs_days': lambda x: max(x) - min(x)  \n",
    "    },\n",
    "    'smart_9_raw': {\n",
    "        #'min_runtime': 'min'\n",
    "        'runtime': 'max',  \n",
    "        'obs_runtime': lambda x: max(x) - min(x)}    \n",
    "    }\n",
    "\n",
    "    by_model_serial = data.groupby(['model', 'serial_number']).agg(aggregations)#.reset_index()\n",
    "    by_model_serial.columns = by_model_serial.columns.droplevel()\n",
    "    models = by_model_serial.index.levels[0].tolist()\n",
    "    ### The annualized failure rate is: 100 * Failures/(Drive Days/365)\n",
    "    summary_cols = ['model', 'size', 'count', 'days', 'obs_days','runtime', 'obs_runtime','failure_rate', 'obs_failure_rate', \n",
    "                    'percent_total', 'failures']\n",
    "    #summary_cols = ['model', 'size', 'count', 'days', 'obs_days', 'max_days','min_days', \n",
    "    #                'min_runtime','runtime', 'obs_runtime','failure_rate', 'obs_failure_rate', \n",
    "    #                'percent_total', 'failures']\n",
    "    summary = pd.DataFrame([], columns=summary_cols)\n",
    "    ntot_drives = float(data['serial_number'].value_counts().count())\n",
    "    \n",
    "    for i in models:\n",
    "        tmp = by_model_serial.xs(i)\n",
    "        drive_count = len(tmp)\n",
    "        nfailures = np.sum(tmp['failure'])\n",
    "        obs_runtime =  np.sum(tmp['obs_runtime'])\n",
    "        runtime = np.sum(tmp['runtime'])\n",
    "        if runtime != 0:\n",
    "            failrate_runtime = 100.0 * nfailures/(runtime/8760.0) \n",
    "        else: \n",
    "            failrate_runtime = 'NaN'\n",
    "        if obs_runtime != 0:\n",
    "            failrate_obs_runtime = 100.0 * nfailures/(obs_runtime/8760.0) \n",
    "        else: \n",
    "            failrate_obs_runtime  = 'NaN'\n",
    "            \n",
    "        #print(np.nansum(tmp['days']))\n",
    "        #print(np.nansum(tmp['obs_days'])) \n",
    "        df_tmp = pd.DataFrame([[i, stats.mode(tmp['mean_cap'])[0][0], drive_count, np.nansum(tmp['days']), np.nansum(tmp['obs_days']),\n",
    "                                runtime, obs_runtime, failrate_runtime, failrate_obs_runtime, \n",
    "                                drive_count/ntot_drives, nfailures]], columns=summary_cols)\n",
    "\n",
    "        summary = summary.append(df_tmp, ignore_index=True)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = start_logger()\n",
    "\n",
    "for data_dir in DATA_FOLDERS:\n",
    "    data_path = DATA_DIR + data_dir + \"/\"\n",
    "    logger.info(\" * * *\")\n",
    "    logger.info(\"Loading and working with: %s\" % data_path)\n",
    "    hd = pd.DataFrame()\n",
    "    for data_file in os.listdir(data_path):\n",
    "        if data_file.split('.')[1] == 'csv':\n",
    "            if TEST:\n",
    "                temp = pd.read_csv(data_path + data_file, header=0, nrows=10000)\n",
    "                temp = sift_data(temp)\n",
    "            else:\n",
    "                temp = pd.read_csv(data_path + data_file, header=0)\n",
    "                temp = sift_data(temp)\n",
    "            hd = hd.append(temp)\n",
    "    hd.capacity_bytes = hd.capacity_bytes.map(tb_capacity)\n",
    "    hd.rename(columns={'capacity_bytes': 'capacity_tb'}, inplace=True)\n",
    "    hd['date'] = hd['date'].apply(pd.to_datetime)\n",
    "    logger.info(\"There are %d unique drives. \" % hd['serial_number'].value_counts().count())\n",
    "    logger.info(\"There are %d unique models. \" % hd['model'].value_counts().count())\n",
    "    logger.info(\"There are %d unique dates. \" % hd['date'].value_counts().count())\n",
    "    logger.info(\"There are %d failures.\" % hd['failure'].sum())\n",
    "    summary = summarize(hd)\n",
    "    \n",
    "    summary.to_csv(OUTPUT_DIR + \"/\" + data_dir + \".csv\", index = False)\n",
    "    #summary.to_csv(OUTPUT_DIR + \"/\" + THE_TIME + \"-\" + data_dir + \".csv\", index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
