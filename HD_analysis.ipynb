{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disk Failure Analysis\n",
    " * A preliminary analyis of hard drives 2016 from Backblaze. This dataset contains basic hard drive information and 90 columns of raw and normalized values of 45 different S.M.A.R.T. statistics. Each row represents a daily snapshot of one hard drive.\n",
    " * code source: https://github.com/bastidas/disk_analysis\n",
    " * data source: https://www.backblaze.com/b2/hard-drive-test-data.html\n",
    " * alexander.bastidas.fry@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import struct\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "dir = \"/home/alex/Downloads/data_Q1_2016/\"\n",
    "hd = pd.DataFrame()\n",
    "for file in os.listdir(dir): # loading 78 indivual data files\n",
    "    temp = pd.read_csv(dir+file, header=0,nrows=20000) # my laptop is struggling to handle more rows\n",
    "    hd = hd.append(temp)\n",
    "print(\"Shape of hd data: \", np.shape(hd))\n",
    "print(\"There are %d unique drives. \" % hd['serial_number'].value_counts().count())\n",
    "print(\"There are %d unique models. \" % hd['model'].value_counts().count())\n",
    "print(\"There are %d unique dates. \" % hd['date'].value_counts().count())\n",
    "print(\"There are %d failures.\" % hd['failure'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select hard drive models for which we have at least on failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hd_models = hd.groupby(\"model\").agg({\"failure\": np.sum, \"serial_number\": pd.Series.nunique})\n",
    "hd_models_index = hd_models.index.tolist()\n",
    "hd_failure_count = hd_models.failure.tolist()\n",
    "n_required = 5 \n",
    "for q in range(len(hd_models)):\n",
    "    if hd_failure_count[q] < n_required:\n",
    "        #print(hd_models_index[q])\n",
    "        hd = hd[hd.model != hd_models_index[q]]\n",
    "hd_models = hd.groupby(\"model\").agg({\"failure\": np.sum, \"serial_number\": pd.Series.nunique})\n",
    "print('hd models', hd_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tb_capacity(x):\n",
    "    \"\"\"\n",
    "    1 gig is np.power(2, 30) bytes, but maybe it is 10^9 bytes. Who you asking? Whatever.\n",
    "    \"\"\"\n",
    "    tb = np.power(10, 12)\n",
    "    #if not math.isnan(x):\n",
    "    return x/tb\n",
    "\n",
    "hd.capacity_bytes = hd.capacity_bytes.map(tb_capacity)\n",
    "hd.rename(columns={'capacity_bytes': 'capacity_tb'}, inplace=True)\n",
    "print(hd.groupby('capacity_tb').size()) # looks like the capacity is stored irregularly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hd.loc[:, 'date'] = pd.to_datetime(hd.loc[:, 'date'])\n",
    "hd.loc[:, 'date'] = hd.loc[:, 'date'] .dt.dayofyear\n",
    "hd.rename(columns={'date': 'day_of_year'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = hd['model'].value_counts().index.tolist()\n",
    "hd_by_model =[]\n",
    "aggregations = {\n",
    "    'failure': {\n",
    "     'failure': 'sum'\n",
    "    },  \n",
    "    'serial_number':{\n",
    "         'unique_serial': pd.Series.nunique   \n",
    "        }\n",
    "}\n",
    "\n",
    "for q in models: #for each model get its stats per day\n",
    "    ags = hd[hd['model'] == q].groupby(\"day_of_year\").agg(aggregations)#.reset_index()\n",
    "    ags.columns = ags.columns.droplevel()\n",
    "    hd_by_model.append(ags)\n",
    "\n",
    "tot_failures = []    \n",
    "for q in range(len(models)):\n",
    "    tot_failures.append( np.cumsum(hd_by_model[q]['failure']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in range(len(hd_by_model)):\n",
    "    failure_rate = 100*365*np.mean(hd_by_model[n].failure/hd_by_model[n].unique_serial)\n",
    "    print(\"%s fails at an annual percentage rate of %s percent.\" % (models[n],np.round(failure_rate,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sn_colors = sns.color_palette(\"hls\", 8)\n",
    "tmark=\"\"\n",
    "fig = plt.figure(figsize=(1,1))\n",
    "ax = fig.add_subplot(111)\n",
    "for tcolor, umn in zip(sn_colors, range(len(models))):\n",
    "    ax.plot([None], [None], marker=tmark, color=tcolor, lw=1.0,label = models[umn])\n",
    "leg = plt.legend(loc='center', shadow=False, frameon=True, ncol=2, borderaxespad=0.)\n",
    "leg.get_frame().set_linewidth(0.5)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(221)\n",
    "for tcolor, umn, in zip(sn_colors, range(len(models))):\n",
    "    ax.plot(hd_by_model[umn].index, hd_by_model[umn].failure, marker=tmark, color=tcolor, lw=1.0)\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('# failures')\n",
    "ax.set_xlim(0)\n",
    "ax.set_ylim(0)\n",
    "ax = fig.add_subplot(222)\n",
    "for tcolor, umn in zip(sn_colors, range(len(models))):\n",
    "    ax.semilogy(hd_by_model[umn].index, hd_by_model[umn].unique_serial, marker=tmark, color=tcolor, lw=1.0)\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('# of unique drives')\n",
    "ax = fig.add_subplot(223)\n",
    "for tcolor, umn in zip(sn_colors, range(len(models))):\n",
    "    ax.plot(hd_by_model[umn].index, tot_failures[umn], marker=tmark, color=tcolor, lw=1.0)\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('cumulative # failures')\n",
    "ax = fig.add_subplot(224)\n",
    "for tcolor, umn in zip(sn_colors, range(len(models))):\n",
    "    ax.plot(hd_by_model[umn].index, tot_failures[umn]/hd_by_model[umn].unique_serial, marker=tmark, color=tcolor, lw=1.0)\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Cumulative Failure Fraction')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the median survival time of a hard drive?\n",
    "* Survival analysis for this, in particular the [lifelines package](http://lifelines.readthedocs.io/en/latest/). Note that this data is truncated and censored, what that means is that we need to use careful statistics to predict reasonable survival times and reasonable confidence levels of those predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_map = dict(zip(hd_models.index.tolist(),range(len(hd_models)))) # map the model names to integers for convenience\n",
    "hd = hd.replace({'model': model_map})\n",
    "print(model_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seagates = pd.DataFrame.copy(hd[hd['model'] == 'ST4000DM000']) # selected the most common seagate model\n",
    "aggregations = {\n",
    "    'day_of_year': { \n",
    "        'first_date': 'min',  \n",
    "        'last_date': 'max', \n",
    "        'uncensored_days': 'count',\n",
    "        'days': lambda x: max(x) - min(x)  \n",
    "    },\n",
    "    'smart_9_raw': { # smart 9 is the disk uptime\n",
    "        'runtime_max': 'max',  \n",
    "        'runtime_min': 'min',\n",
    "        'uptime': lambda x: max(x) - min(x)  \n",
    "    },\n",
    "    'model':{\n",
    "       'model_count': 'count',\n",
    "       'model': 'mean'\n",
    "    },\n",
    "    'failure': {\n",
    "     'failure': 'sum'\n",
    "    }  \n",
    "}\n",
    "survival = hd.groupby('serial_number').agg(aggregations).reset_index()\n",
    "survival.columns = survival.columns.droplevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "survival = survival.loc[survival['failure'] <=1 ]\n",
    "survival['runtime_max'] = survival['runtime_max']/8760.0 # hrs to days\n",
    "survival['runtime_min'] = survival['runtime_min']/8760.0\n",
    "print(survival['model'].value_counts())\n",
    "del survival['model_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaplan-Meier Modeling\n",
    "\n",
    " *    Kaplan-Meier is a non-parametric non-generalising maximum-likelihood estimate of the survival function, S(t), which is the probability of survival until time t.\n",
    " *   The data is left truncated: hard drives enter study interval at different times. So we use the 'entry' keyword (Did I interpret the lifelines documentation right?)\n",
    " *   If we had a parametric form of the survival function we could predict new unseen data and predict the relative impact on survival of various hard drive attributes...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lifelines as sa\n",
    "km = sa.KaplanMeierFitter()\n",
    "km.fit(durations=survival['runtime_max'], event_observed=survival['failure'], entry=survival['runtime_min'])\n",
    "S = km.survival_function_\n",
    "ax = km.plot(title=\"Kaplan Meier fit\", legend=False)\n",
    "ax.annotate('half-life', color='black', xy=(0.01,0.5), xycoords='axes fraction', xytext=(10,4), textcoords='offset points')\n",
    "ax.axhline(.5, ls='--', lw=1.0, color='black')\n",
    "ax.set_ylim([0,1])\n",
    "#ax.set_xlim([0,6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hazard rates with Nelson-Aalen\n",
    " * The failure rate is the total number of failures within a population, divided by the total time expended by that population, during a particular measurement interval.\n",
    " * The hazard function or hazard rate is the failure rate calculated instantaneously.  \n",
    " * The cumulative hazard curve is a basic tool: it is the sum of failure rate estimates so it is much more stable than the point-wise instananeous estimates.\n",
    " * The hazard curve has a catch: the derivation involves a smoothing kernel smoother applied to the differences of the cumulative hazard curve), and thus it has a free parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lifelines import NelsonAalenFitter\n",
    "naf = NelsonAalenFitter()\n",
    "seagate = survival.loc[survival['model'] == model_map['ST4000DM000']]\n",
    "naf.fit(seagate['runtime_max'], seagate['failure'], entry=seagate['runtime_min'], label='Seagate ST4000DM000')\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, squeeze=False, sharex=True, sharey=True)\n",
    "naf.plot(ax=axes[0,0],title=\"Cumulative Hazard Rate\")\n",
    "plt.show()\n",
    "\n",
    "haz = naf.smoothed_hazard_\n",
    "smoothing_bandwidth_time=1.0\n",
    "q = haz(smoothing_bandwidth_time)\n",
    "mean_haz = np.mean( q[q.columns[0]])\n",
    "\n",
    "ax = naf.plot_hazard(bandwidth=smoothing_bandwidth_time, title = \"Hazard Rate\")\n",
    "ax.axhline(mean_haz , ls='--', lw=1.0, color='black')\n",
    "haz_str = str(100*np.round(mean_haz,4)) + \"%\"\n",
    "ax.annotate(haz_str, color='black', xy=(0.01,mean_haz), xytext=(10,4), textcoords='offset points')\n",
    "plt.show()\n",
    "print(\"The failure rate for these seagate drives is %s\" % haz_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That was a lot of work to get basically the same failure rate that we calculated all the way back at the begining. Further survival regression analyis would be useful, but lets move on from survival analysis for now and ask another question.\n",
    "\n",
    "##  Predict which hard drives will fail given the hard drive statistics?\n",
    " * So there isn't a lot of information about drive failures, but can we at least find what hard drive diagnostics (the smart values) are preddictive of a hard drive failure?\n",
    " * Most of the data columns are S.M.A.R.T. values that can vary in meaning based on the manufacturer and model. For this reason at this time limit the more in depth analysis to the most common identical drive model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = hd.failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get just the raw columns and ignore columns with bad data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clip_by_column_txt(df, col_txt):\n",
    "    for ikey in df.keys():\n",
    "        if col_txt not in ikey:\n",
    "            del df[ikey]\n",
    "\n",
    "clip_by_column_txt(hd,\"raw\") # select raw columns only\n",
    "hd = hd.dropna(axis=1)\n",
    "#sel = VarianceThreshold(threshold=.01)  # remove the features with variance below threshold.\n",
    "#sel.fit_transform(hd)\n",
    "#print(hd.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are a few ways you could decide to clip the important features... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use forest classifier to find important features\n",
    "forest = ExtraTreesClassifier(n_estimators=250, random_state=0)\n",
    "forest.fit(hd, y)\n",
    "importance = forest.feature_importances_\n",
    "std_err = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "# get the indices and sort the labels of the important features\n",
    "indices = np.argsort(importance)[::-1]\n",
    "x_labels = [x for (y, x) in sorted(zip(importance, hd.columns))][::-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the plot the dashed line is the median of the importance which is the chosen threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clip the features to the most important ones\n",
    "thresh = np.median(importance)\n",
    "clip_importance = importance[np.where(importance >= thresh)]\n",
    "clip_indices = indices[0:len(clip_importance)]\n",
    "clip_x_labels = x_labels[0:len(clip_importance)]\n",
    "for i in range(len(clip_indices)):\n",
    "    print(\"%d. clipped feature %s (%f)\" % (i + 1, clip_x_labels[i], importance[clip_indices[i]]))\n",
    "sn_colors = sns.color_palette()\n",
    "plt.figure()\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(len(clip_indices)), importance[clip_indices], color=sn_colors[1],\n",
    "        yerr=std_err[clip_indices], align=\"center\")\n",
    "# Plot the mean line, below using SelectFromModel we will clip features below this value\n",
    "plt.axhline(np.median(importance), ls='--', lw=2.0, color='black', alpha=99)\n",
    "plt.xticks(range(len(clip_indices)), clip_x_labels, rotation=80)\n",
    "plt.xlim([-1, len(clip_indices)])\n",
    "plt.ylim([0, .5])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
