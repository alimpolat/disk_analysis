{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HD_Survival_Analysis\n",
    "generates fits from lifelines package, saves those fits as data files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lifelines as lil\n",
    "from lifelines import KaplanMeierFitter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from bokeh.plotting import ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "from bokeh.layouts import row\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import Range1d\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST = True\n",
    "DATA_DIR = \"data/\"\n",
    "DATA_FOLDERS = [\"2014\",\"2015\",\"data_Q1_2016\",\"data_Q2_2016\",\"data_Q3_2016\",\"data_Q4_2016\"]\n",
    "DATA_FOLDERS = [\"Q1_2014\",\"Q1_2014\",\"Q1_2014\",\"Q1_2014\",\n",
    "                \"Q1_2015\",\"Q2_2015\",\"Q2_2015\",\"Q4_2015\",\n",
    "                \"data_Q1_2016\",\"data_Q2_2016\",\"data_Q3_2016\",\"data_Q4_2016\"]\n",
    "SUMMARY_DIR = \"summary_data/\"\n",
    "PERCENT_TOTAL_REQ = .05\n",
    "FAILURE_RATE_REQ = 5\n",
    "MIN_NUMBER_REQ = 100\n",
    "INPUT_DIR = \"survival_data\"\n",
    "OUTPUT_DIR = \"data/survival/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_models = []\n",
    "for df in DATA_FOLDERS:\n",
    "    print(SUMMARY_DIR + df + '.csv')\n",
    "    summary_dats = pd.read_csv(SUMMARY_DIR + df + '.csv', header=0, nrows=200)\n",
    "    summary_dats = summary_dats.sort_values(by=\"percent_total\", ascending=False)\n",
    "    clipped1 = summary_dats[summary_dats['percent_total'] >= PERCENT_TOTAL_REQ]\n",
    "    clipped2 = summary_dats[(summary_dats['failure_rate'] >= FAILURE_RATE_REQ) & (summary_dats['drive_count'] >= MIN_NUMBER_REQ)]\n",
    "    [target_models.append(m) for m in clipped1['model']]\n",
    "    [target_models.append(m) for m in clipped2['model']]\n",
    "\n",
    "unique_target_models = np.unique(target_models)\n",
    "print(unique_target_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "survival_data = []\n",
    "km_data = []\n",
    "for model in unique_target_models:\n",
    "    model_ns = model.replace(\" \", \"_\")\n",
    "    pf = INPUT_DIR + \"/survival_\" +  model_ns + '.csv'# + \"data_Q2_2016.csv\"\n",
    "    print(pf)\n",
    "    tmp = pd.read_csv(pf, header=0)\n",
    "    yr = 365. * 24.\n",
    "    tmp['runtime_max'] = tmp['runtime_max']/yr\n",
    "    tmp['uptime'] = tmp['uptime']/yr\n",
    "    tmp['runtime_min'] = tmp['runtime_min']/yr\n",
    "    survival_data.append(tmp)\n",
    "    km = lil.KaplanMeierFitter()\n",
    "    try:\n",
    "        km.fit(durations=tmp['runtime_max'], event_observed=tmp['failure'], entry=tmp['runtime_min'])\n",
    "        km_data.append(km)\n",
    "        s = km.survival_function_\n",
    "        ci = km.confidence_interval_\n",
    "        i = s.index\n",
    "        s = s.iloc[:,0].values\n",
    "        u = ci.iloc[:,0].values\n",
    "        l = ci.iloc[:,1].values\n",
    "        tmp = pd.DataFrame(np.asarray([i,s,u,l]).T, columns = ['time','surv', 'surv_upper', 'surv_lower'])\n",
    "        tmp.to_csv(OUTPUT_DIR + \"kmf_\" + model_ns + \".csv\", index = False)\n",
    "    except:\n",
    "        print(\"Probably too few early truncation times and too many events. Try BreslowFlemingHarringtonFitter?\")\n",
    "        print(\"Skipping ...\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def km_bokeh_plot(kms, models):\n",
    "    surv_plt1 = figure(title=\"Survival Analysis\", tools=['hover,box_zoom,wheel_zoom,save,reset'])\n",
    "    hover = surv_plt1.select(dict(type=HoverTool))\n",
    "    hover.tooltips = [\n",
    "        (\"Model \", \"@model\"),\n",
    "        (\"Time \", \"@timeline\"),\n",
    "        (\"survival fraction \", \"@km_surv\"),\n",
    "        (\"upper 95% bound \", \"@surv_upper\"),\n",
    "        (\"lower 95% bound \", \"@surv_lower\")\n",
    "        ]\n",
    "    if len(kms) > 1:\n",
    "        hover.mode = 'mouse'\n",
    "    else: \n",
    "        hover.mode = 'vline'\n",
    "    colors = ['#1a1334', '#03c383', '#fbbf45', '#ed0345' ,  '#26294a', '#aad962', '#01545a', '#ef6a32','#017351',\n",
    "              '#a12a5e', '#710162', '#110141']\n",
    "    n = 0\n",
    "    for km in kms:\n",
    "        s = km.survival_function_\n",
    "        ci = km.confidence_interval_\n",
    "        time = s.index\n",
    "        surv = s.iloc[:,0].values\n",
    "        surv_upper = ci.iloc[:,0].values\n",
    "        surv_lower = ci.iloc[:,1].values\n",
    "        \n",
    "        band_x = np.append(time, time[::-1])\n",
    "        band_y = np.append(surv_upper, surv_lower[::-1])\n",
    "        source = ColumnDataSource(\n",
    "            data=dict(\n",
    "                timeline=[i for i in time],\n",
    "                km_surv=[i for i in surv],\n",
    "                model=[models[n] for i in time],\n",
    "                surv_lower=[i for i in surv_lower],\n",
    "                surv_upper=[i for i in surv_upper]\n",
    "            )\n",
    "        )\n",
    "        surv_plt1.patch(band_x, band_y, color=colors[n], fill_alpha=0.2)\n",
    "        surv_plt1.line('timeline', 'km_surv', line_width = 2, alpha=.8, source = source, legend=models[n], color=colors[n])\n",
    "        n += 1\n",
    "    surv_plt1.xaxis.axis_label = 'Time (Years)'\n",
    "    surv_plt1.yaxis.axis_label = 'Kaplan-Meier Estimation (survival fraction)'\n",
    "    surv_plt1.grid.grid_line_alpha = 0\n",
    "    surv_plt1.ygrid.band_fill_color = \"grey\"\n",
    "    surv_plt1.ygrid.band_fill_alpha = 0.2\n",
    "    surv_plt1.x_range.range_padding = 0\n",
    "    surv_plt1.legend.location = \"bottom_left\"\n",
    "    surv_plt1.plot_height = 500\n",
    "    surv_plt1.plot_width = 700\n",
    "    surv_plt1.min_border_left = 80\n",
    "    surv_plt1.outline_line_width = 1\n",
    "    surv_plt1.outline_line_alpha = 0.3\n",
    "    surv_plt1.y_range = Range1d(0.0, 1.02)\n",
    "    show(surv_plt1)\n",
    "         \n",
    "#km_bokeh_plot(km_data, unique_target_models)\n",
    "km_bokeh_plot(km_data[0:6], unique_target_models[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hazard rates with Nelson-Aalen\n",
    " * The failure rate is the total number of failures within a population, divided by the total time expended by that population, during a particular measurement interval.\n",
    " * The hazard function or hazard rate is the failure rate calculated instantaneously.  \n",
    " * The cumulative hazard curve is a basic tool: it is the sum of failure rate estimates so it is much more stable than the point-wise instananeous estimates.\n",
    " * The hazard curve has a catch: the derivation involves a smoothing kernel smoother applied to the differences of the cumulative hazard curve), and thus it has a free parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def avg_failrate(t, chaz, model_ns):\n",
    "    yrs = [1., 2., 3., 4., 5., 6.]\n",
    "    #avg_frate = []\n",
    "    #delta_frate = []\n",
    "    avg_frate = np.zeros(len(yrs))-1\n",
    "    delta_frate = np.zeros(len(yrs))-1\n",
    "    index = []\n",
    "    i = -1\n",
    "    for yr in yrs:\n",
    "        mt = np.min(np.abs(t-yr))\n",
    "        i += 1\n",
    "        if mt>.2:\n",
    "            break\n",
    "        tmp = np.where(np.abs(t-yr) == mt)[0][0]\n",
    "        index.append(tmp)\n",
    "        print \"average failure rate at\", yr, \" year is \", chaz[index[i]]/yr\n",
    "        #avg_frate.append(chaz[index[i]]/yr)\n",
    "        avg_frate[i] = chaz[index[i]]/yr\n",
    "        if i == 0:\n",
    "            #delta_frate.append(chaz[index[i]]/yr)\n",
    "            delta_frate[i] = chaz[index[i]]/yr\n",
    "        if i >= 1:\n",
    "            dfrate =  (chaz[index[i]] - chaz[index[i-1]]) / (yrs[i]-yrs[i-1])\n",
    "            print \"average failure between \", yrs[i], \" year and \", yrs[i-1], \"year is \", dfrate\n",
    "            #delta_frate.append(dfrate)\n",
    "            delta_frate[i] = dfrate\n",
    "    #print avg_frate\n",
    "    #print delta_frate \n",
    "    tmp = pd.DataFrame(np.asarray([yrs, avg_frate, delta_frate]).T, \n",
    "                           columns = ['yr', 'avg_fail_rate', 'delta_avg_fail_rate'])\n",
    "    tmp.to_csv(OUTPUT_DIR + \"avg_\" + model_ns + \".csv\", index = False)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lifelines import NelsonAalenFitter\n",
    "for model in unique_target_models:\n",
    "    model_ns = model.replace(\" \", \"_\")\n",
    "    pf = INPUT_DIR + \"/survival_\" +  model_ns + '.csv'# + \"data_Q2_2016.csv\"\n",
    "    print(pf)\n",
    "    tmp = pd.read_csv(pf, header=0)\n",
    "    yr = 365. * 24.\n",
    "    tmp['runtime_max'] = tmp['runtime_max']/yr\n",
    "    tmp['uptime'] = tmp['uptime']/yr\n",
    "    tmp['runtime_min'] = tmp['runtime_min']/yr\n",
    "    hz = NelsonAalenFitter()\n",
    "    #while True:\n",
    "    try:\n",
    "        hz.fit(tmp['runtime_max'], tmp['failure'], entry=tmp['runtime_min'])\n",
    "        chaz = hz.cumulative_hazard_\n",
    "        t = chaz.index\n",
    "        chaz = chaz.iloc[:,0].values\n",
    "        \n",
    "        shaz = hz.smoothed_hazard_\n",
    "        smoothing_bandwidth_time=.5\n",
    "        shaz50 = shaz(smoothing_bandwidth_time)\n",
    "        shaz50 = shaz50.iloc[:,0].values\n",
    "        \n",
    "        shaz = hz.smoothed_hazard_\n",
    "        smoothing_bandwidth_time=.25\n",
    "        shaz25 = shaz(smoothing_bandwidth_time)\n",
    "        shaz25 = shaz25.iloc[:,0].values\n",
    "        \n",
    "        shaz = hz.smoothed_hazard_\n",
    "        smoothing_bandwidth_time=1.0\n",
    "        shaz100 = shaz(smoothing_bandwidth_time)\n",
    "        shaz100 = shaz100.iloc[:,0].values\n",
    "        \n",
    "        #print t\n",
    "        avg_failrate(t.values, chaz, model_ns)\n",
    "            \n",
    "            \n",
    "        tmp = pd.DataFrame(np.asarray([t, chaz, shaz25, shaz50, shaz100]).T, \n",
    "                           columns = ['time', 'cumulative_hazard', 'smoothed_hazard25', \n",
    "                                      'smoothed_hazard50', 'smoothed_hazard100'])\n",
    "        tmp.to_csv(OUTPUT_DIR + \"naf_\" + model_ns + \".csv\", index = False)\n",
    "    except:\n",
    "        print(\"Well that didn't work. And this message will hardly help, Ha, who knows why.\")\n",
    "        print(\"Skipping ...\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_class_members(klass):\n",
    "    ret = dir(klass)\n",
    "    if hasattr(klass,'__bases__'):\n",
    "        for base in klass.__bases__:\n",
    "            ret = ret + get_class_members(base)\n",
    "    return ret\n",
    "\n",
    "#get_class_members(hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chaz = hz.cumulative_hazard_\n",
    "t = chaz.index\n",
    "chaz = chaz.iloc[:,0].values\n",
    "plt.plot(t, chaz)#\"Cumulative Hazard Rate\"\n",
    "plt.show()\n",
    "\n",
    "shaz = hz.smoothed_hazard_\n",
    "smoothing_bandwidth_time=.2\n",
    "shaz = shaz(smoothing_bandwidth_time)\n",
    "t = shaz.index\n",
    "shaz = shaz.iloc[:,0].values\n",
    "plt.plot(t, shaz)#\"Smoothed Hazard Rate\"\n",
    "\n",
    "mean_shaz = np.mean( shaz)\n",
    "plt.axhline(mean_shaz , ls='--', lw=1.0, color='black')\n",
    "shaz_str = str(100*np.round(mean_shaz,4)) + \"%\"\n",
    "plt.annotate(shaz_str, color='black', xy=(0.01,mean_shaz), xytext=(10,4), textcoords='offset points')\n",
    "plt.show()\n",
    "print(\"The failure rate for the drive is %s\" % shaz_str)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
