{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named wtte.objectives.tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-18975e8c21e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwtte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjectives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloglik_discrete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetapenalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#output_notebook()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named wtte.objectives.tensorflow"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from wtte.objectives.tensorflow import loglik_discrete, betapenalty\n",
    "#output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very telling to visualize the failures. What we can observe: we can observe the last time that the hardrive was seen in the data set. If the hard drive is no longer in the data set, it has failed, or has it just been removed from the data set? Our U matrix will be a mask the denotes censored drives, and most of our drives are censored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pf = \"../survival_data/survival_ST4000DM000.csv\"\n",
    "pf  = \"../survival_data/survival_Hitachi_HDS723030ALA640.csv\"\n",
    "        df = pd.read_csv(pf, header=0)\n",
    "        day = 24.0\n",
    "        df_failures_only = df[df['failure'] ==1]\n",
    "        ttf = df_failures_only['runtime_max']/day\n",
    "        ttf_cen = df['runtime_max']/day\n",
    "        et = df_failures_only['runtime_min']/day\n",
    "        et_cen = df['runtime_min']/day\n",
    "\n",
    "        u_train = np.abs(df_failures_only['failure']-1)\n",
    "        u_train_cen = np.abs(df['failure']-1)\n",
    "\n",
    "        sp1(ttf,ttf_cen, et, et_cen)\n",
    "        sp2(ttf, ttf_cen, et, et_cen)\n",
    "        p1(ttf, ttf_cen, et, et_cen, u_train, u_train_cen, plt_sorted = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survival_ST31500341AS.csv\n",
      "125\n",
      "survival_Hitachi_HDS5C4040ALE630.csv\n",
      "58\n",
      "survival_ST3160316AS.csv\n",
      "survival_Hitachi_HDT725025VLA380.csv\n",
      "survival_ST32000542AS.csv\n",
      "survival_ST1000LM024_HN.csv\n",
      "survival_ST2000DM001.csv\n",
      "survival_ST2000VN000.csv\n",
      "survival_ST250LT007.csv\n",
      "survival_ST33000651AS.csv\n",
      "survival_SAMSUNG_HD103UJ.csv\n",
      "survival_Hitachi_HDS722020ALA330.csv\n",
      "202\n",
      "survival_HGST_HMS5C4040BLE640.csv\n",
      "56\n",
      "survival_ST4000DX002.csv\n",
      "survival_Hitachi_HDS723020BLA642.csv\n",
      "survival_TOSHIBA_MD04ABA500V.csv\n",
      "survival_ST9250315AS.csv\n",
      "survival_Hitachi_HDS5C3030BLE630.csv\n",
      "survival_ST320005XXXX.csv\n",
      "survival_ST2000DL001.csv\n",
      "survival_TOSHIBA_MD04ABA400V.csv\n",
      "survival_ST6000DM001.csv\n",
      "survival_ST2000DL003.csv\n",
      "survival_Hitachi_HDS724040ALE640.csv\n",
      "survival_ST9320325AS.csv\n",
      "survival_ST250LM004_HN.csv\n",
      "survival_ST320LT007.csv\n",
      "85\n",
      "survival_TOSHIBA_MQ01ABF050.csv\n",
      "survival_ST4000DX000.csv\n",
      "36\n",
      "survival_Hitachi_HDS723030ALA640.csv\n",
      "63\n",
      "survival_Hitachi_HDS5C3030ALA630.csv\n",
      "110\n",
      "survival_ST1500DM003.csv\n",
      "survival_ST31500541AS.csv\n",
      "274\n",
      "survival_ST500LM012_HN.csv\n",
      "28\n",
      "survival_ST1500DL001.csv\n",
      "survival_HGST_HUH728080ALE600.csv\n",
      "survival_TOSHIBA_DT01ACA300.csv\n",
      "survival_ST8000NM0055.csv\n",
      "survival_SAMSUNG_HD154UI.csv\n",
      "survival_ST1500DL003.csv\n",
      "39\n",
      "survival_ST3500320AS.csv\n",
      "survival_HGST_HMS5C4040ALE640.csv\n",
      "107\n",
      "survival_ST3000DM001.csv\n",
      "1454\n",
      "survival_HGST_HDS5C4040ALE630.csv\n",
      "survival_ST4000DM000.csv\n",
      "1762\n",
      "survival_ST6000DX000.csv\n",
      "45\n",
      "survival_Hitachi_HDT721010SLA360.csv\n",
      "survival_HGST_HDS724040ALE640.csv\n",
      "survival_Hitachi_HDS723030BLE640.csv\n",
      "survival_ST8000DM002.csv\n",
      "48\n",
      "survival_ST3160318AS.csv\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"../survival_data/\"\n",
    "for data_file in os.listdir(DATA_FOLDER ):\n",
    "    if data_file.split('.')[1] == 'csv':\n",
    "        df = pd.read_csv(DATA_FOLDER + data_file, header=0)\n",
    "        day = 24.0\n",
    "        df_failures_only = df[df['failure'] ==1]\n",
    "        ttf = df_failures_only['runtime_max']/day\n",
    "        ttf_cen = df['runtime_max']/day\n",
    "        et = df_failures_only['runtime_min']/day\n",
    "        et_cen = df['runtime_min']/day\n",
    "\n",
    "        u_train = np.abs(df_failures_only['failure']-1)\n",
    "        u_train_cen = np.abs(df['failure']-1)\n",
    "\n",
    "        if len(ttf) > 20:\n",
    "            sp1(ttf,ttf_cen, et, et_cen, save_fig = data_file)\n",
    "            sp2(ttf, ttf_cen, et, et_cen, save_fig = data_file)\n",
    "            p1(ttf, ttf_cen, et, et_cen, u_train, u_train_cen, plt_sorted = False, save_fig = data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sp1(ttf,ttf_cen, et, et_cen, save_fig = \"\"):\n",
    "    sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "    sns.despine(top=True)\n",
    "    binwidth = 30\n",
    "    bw = 90\n",
    "    bins_f = np.arange(min(ttf), max(ttf) + binwidth, binwidth)\n",
    "    bins_c = np.arange(min(ttf_cen), max(ttf_cen) + binwidth, binwidth)\n",
    "\n",
    "    f, axes = plt.subplots(2, 1, figsize=(14, 7), sharex=True)\n",
    "    sns.distplot(ttf, color=\"red\", ax=axes[0],  kde_kws={'bw':bw}, bins = bins_f)\n",
    "    sns.distplot(et, color=\"grey\", ax=axes[0],  kde_kws={'bw':bw}, bins = bins_f)\n",
    "    sns.distplot(ttf_cen, color=\"blue\", ax=axes[1], kde_kws={'bw':bw}, bins = bins_c)\n",
    "    sns.distplot(et_cen, color=\"grey\", ax=axes[1], kde_kws={'bw':bw}, bins = bins_c)\n",
    "    axes[0].set_xlim([0, 1.02*ttf_cen.max()])\n",
    "    axes[1].set_xlim([0, 1.02*ttf_cen.max()])\n",
    "    axes[0].set_ylabel('Normalized Failure Count')\n",
    "    axes[1].set_ylabel(\"Normalized Censor Count\")\n",
    "    axes[1].set_xlabel('Time')\n",
    "    axes[0].set_xlabel('')\n",
    "    plt.tight_layout()\n",
    "    if save_fig != \"\":\n",
    "        f.savefig(save_fig.split('.')[0] + '_a.png')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sp2(ttf, ttf_cen, et, et_cen, save_fig=\"\"):\n",
    "    binwidth = 30\n",
    "    bw = 90\n",
    "    \n",
    "    f, axes = plt.subplots(2, 1, figsize=(14, 7), sharex=True)\n",
    "    sns.despine(top=True)\n",
    "\n",
    "    bins_f = np.arange(min(ttf), max(ttf) + binwidth, binwidth)\n",
    "    bins_c = np.arange(min(ttf_cen), max(ttf_cen) + binwidth, binwidth)\n",
    "    sns.distplot(ttf, color=\"red\", ax=axes[0], kde_kws={'bw':bw,'cumulative':True}, \n",
    "                 hist_kws=dict(cumulative=True), bins = bins_f)\n",
    "    sns.distplot(et, color=\"grey\", ax=axes[0], kde_kws={'bw':bw,'cumulative':True}, \n",
    "                 hist_kws=dict(cumulative=True), bins = bins_f)\n",
    "\n",
    "    sns.distplot(ttf_cen, color=\"blue\", ax=axes[1], kde=False, \n",
    "                  hist_kws=dict(cumulative=True,normed=False), bins = bins_c)\n",
    "    sns.distplot(et_cen, color=\"grey\", ax=axes[1], kde=False,\n",
    "                  hist_kws=dict(cumulative=True,normed=False), bins = bins_c)   \n",
    "    sns.distplot(ttf, color=\"red\", ax=axes[1], kde=False,\n",
    "                  hist_kws=dict(cumulative=True,normed=False), bins = bins_c)   \n",
    "\n",
    "    axes[0].set_xlim([0, 1.01*ttf_cen.max()])\n",
    "    axes[1].set_xlim([0, 1.01*ttf_cen.max()])\n",
    "    axes[0].set_ylabel('Normalized Cumulative Count')\n",
    "    axes[1].set_ylabel(\"Cumulative Count\")\n",
    "    axes[1].set_xlabel('Time')\n",
    "    axes[0].set_xlabel('')\n",
    "    plt.tight_layout()\n",
    "    if save_fig != \"\":\n",
    "        f.savefig(save_fig.split('.')[0] + '_b.png')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def p1(ttf, ttf_cen, et, et_cen, u_train, u_train_cen, plt_sorted = False, save_fig=\"\"):\n",
    "    sns.set_style(\"white\")\n",
    "    sns.despine(offset=10, trim=True);\n",
    "    colorp =sns.xkcd_palette([\"black\", \"red\", \"blue\"])\n",
    "    sns.set_palette(colorp)\n",
    "    fig_n, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(16, 9))\n",
    "    \n",
    "    if plt_sorted:\n",
    "        ax1.scatter(sorted(ttf), range(len(ttf)), c=colorp[1], s=8, alpha=.9)\n",
    "        \n",
    "        ttf_cen_sorted = sorted(zip(ttf_cen, u_train_cen))\n",
    "        ttf_cen_sorted, u_train_cen_sorted= [a[2] for a in ttf_cen_sorted],[a[1] for a in ttf_cen_sorted]\n",
    "        u_colors_cen_sorted = [colorp[2] if c==1 else colorp[1] for c in u_train_cen_sorted]\n",
    "        ax2.scatter(ttf_cen_sorted, range(len(ttf_cen_sorted)), c = u_colors_cen_sorted, s=8, alpha=.9)\n",
    "    else:\n",
    "        ax1.scatter(ttf, range(len(ttf)), c=colorp[1], s=8, alpha=.9)\n",
    "        \n",
    "        u_colors_cen = [colorp[2] if c==1 else colorp[1] for c in u_train_cen]\n",
    "        ax2.scatter(ttf_cen, range(len(ttf_cen )), c = u_colors_cen, s=8, alpha=.9)\n",
    "        ax2.scatter(et_cen, range(len(ttf_cen )), c = colorp[0], s=8, alpha=.9)\n",
    "    \n",
    "    ax1.set_ylabel('Drive')\n",
    "    ax2.set_ylabel('Drive')\n",
    "    ax1.set_title(\"Failures Only\")\n",
    "    ax2.set_title(\"Censored\")\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_xlim([0, 1.01*ttf_cen.max()])\n",
    "    ax2.set_xlim([0, 1.01*ttf_cen.max()])\n",
    "    if save_fig != \"\":\n",
    "        fig_n.savefig(save_fig.split('.')[0] + '_c.png')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define fit with tensorflow and define a a penalty for beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def betapenalty_alt(b, location = 10.0, growth=20.0, output_collection=(), name=None):\n",
    "    \"\"\"Returns a positive penalty term exploding when beta approaches location.\n",
    "\n",
    "    Adding this term to the loss may prevent overfitting and numerical instability\n",
    "    of large values of beta (overconfidence). Remember that loss = -loglik+penalty\n",
    "\n",
    "    Args:\n",
    "        b:  beta.  Positive nonzero `Tensor` of type `float32`, `float64`.\n",
    "        output_collection: `tuple` of `string`s, name of the collection to\n",
    "                          collect result of this op.\n",
    "        name: `string`, name of the operation.\n",
    "\n",
    "    Returns:\n",
    "        A positive `Tensor` of same shape as `b` being a penalty term\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name, \"weibull_betapenalty\", [b]):\n",
    "        scale = growth/location\n",
    "        penalty = tf.exp(scale*(b-location))\n",
    "        tf.add_to_collection(output_collection, penalty)\n",
    "\n",
    "    return(penalty)\n",
    "\n",
    "\n",
    "def tf_loglik_runner(loglik_fun, tte, tte_mask, n_steps = 2000):    \n",
    "    y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    u_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    \n",
    "    # odd inital a for odd data\n",
    "    a_guess = np.max(tte)\n",
    "    a = tf.exp(tf.Variable(tf.random_normal([1], mean=a_guess, stddev=1.0), name='a_weight'))\n",
    "    b = tf.exp(tf.Variable(tf.random_normal([1],mean= 1.0 , stddev=1.0), name='b_weight'))\n",
    "     \n",
    "    # added this betapenalty\n",
    "    loglik = loglik_fun(a, b, y_,u_) \n",
    "    penalty = betapenalty_alt(b)\n",
    "    loss = -tf.reduce_mean(loglik) + penalty\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.000001)\n",
    "    \n",
    "    train_step = optimizer.minimize(loss)\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        loss_val,_,a_val,b_val= sess.run([loss,train_step, a,b],feed_dict={y_: tte, u_ : tte_mask})\n",
    "        if step % 250 == 0:\n",
    "            print(\"Step: %f, Loss value %s\" % (step,loss_val))\n",
    "            print (\"Step: %f, A: %f B: %f\" % (step,a_val, b_val))\n",
    "    \n",
    "    return a_val, b_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discreteize data and shape it correctly for tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ttf_cen = np.floor(ttf_cen)\n",
    "ttf_cen_s = [[x] for x in ttf_cen]\n",
    "u_train_cen_s =  [[x] for x in  u_train_cen]\n",
    "\n",
    "ttf_f = np.floor(ttf)\n",
    "ttf_f_s = [[x] for x in ttf_f]\n",
    "u_train_f_s =  [[x] for x in  u_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_fit_steps = 2000\n",
    "\n",
    "\n",
    "a_val_cen, b_val_cen = tf_loglik_runner(loglik_discrete, ttf_cen_s, u_train_cen_s, \n",
    "                                        n_steps = n_fit_steps)\n",
    "print (\"Cesnored case A: %f B: %f\" % (a_val_cen[0], b_val_cen[0]))\n",
    "\n",
    "a_val_f, b_val_f = tf_loglik_runner(loglik_discrete, ttf_f_s, u_train_f_s, \n",
    "                                        n_steps = n_fit_steps)\n",
    "print (\"Failures only case A: %f B: %f\" % (a_val_f[0], b_val_f[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total fail. So lets just guess some Weibull params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b_val_f = 1.0\n",
    "a_val_f = np.median(ttf)\n",
    "W_f = np.sort(a_val_f * np.power(-np.log(np.linspace(1e-10,1.,len(ttf))),1.0/b_val_f))\n",
    "W = np.sort(a_val_f * np.power(-np.log(np.linspace(1e-10,1.,len(ttf_cen_sorted))),1.0/b_val_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(16, 9))\n",
    "ax1.scatter(sorted(ttf), range(len(ttf)), c=colorp[1], s=4, alpha=.2)\n",
    "ax1.plot(W_f, range(len(ttf)), c=\"grey\")\n",
    "ax2.scatter(ttf_cen_sorted, range(len(ttf_cen_sorted)), c = u_colors_cen_sorted, s=4, alpha=.2)\n",
    "ax2.plot(W, range(len(ttf_cen)), c=\"grey\")\n",
    "ax2.scatter(ttf_cen_sorted, range(len(ttf_cen_sorted)), c = u_colors_cen_sorted, s=4, alpha=.2)\n",
    "ax1.set_ylabel('Drive')\n",
    "ax2.set_ylabel('Drive')\n",
    "ax1.set_title(\"Failures Only\")\n",
    "ax2.set_title(\"Censored\")\n",
    "ax2.set_xlabel('Time')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_xlim([0, 1.01*ttf_cen.max()])\n",
    "ax2.set_xlim([0, 1.01*ttf_cen.max()])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
