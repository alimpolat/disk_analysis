{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract relevant features and prepare learning.\n",
    " * Most of the data columns are S.M.A.R.T. values that can vary in meaning based on the manufacturer and model. For this reason at this time limit the more in depth analysis to one identical drive model at a time \n",
    " * Sometimes we don't have contiguous sets of data. This is important beause a ML pipline pretty much always requires input vectos/tensors of constant size. For nominal drives this loss doesn't matter much because we have many examples of 'good drives' but for drives that fail it would be best not to lose their statistics. For this reason if we seek DAY_BACK contiguous days, but can only get DAY_BACK - CONTIG_TRYS we would still keep that drive by having CONTIG_TRYS number of non-contiguous days. For example: for a particular disk there are not N=30 straight days of observation before failure, but there are N=1 to 20 days, and then 22 to 153 days, thus we would take 1 to 20 and then 22 to 32. So our total number of obs days is still N=30. This is a kind of data munging hyperparameter, what effects does it have on the results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import struct\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ORIG_COLS = [u'date', u'failure', u'smart_1_raw', u'smart_3_raw', u'smart_4_raw',\n",
    "       u'smart_5_raw', u'smart_7_raw', u'smart_9_raw', u'smart_10_raw',\n",
    "       u'smart_12_raw', u'smart_183_raw', u'smart_184_raw', u'smart_187_raw',\n",
    "       u'smart_188_raw', u'smart_189_raw', u'smart_190_raw', u'smart_191_raw',\n",
    "       u'smart_192_raw', u'smart_193_raw', u'smart_194_raw', u'smart_197_raw',\n",
    "       u'smart_198_raw', u'smart_199_raw', u'smart_240_raw', u'smart_241_raw',\n",
    "       u'smart_242_raw', u'NaNs']\n",
    "DAY_BACK = 60\n",
    "TEST = False\n",
    "DISTANT_DATE = pd.to_datetime(\"2020-01-01\", format='%Y-%m-%d')\n",
    "DAY_BUFFER_FACTOR = 1.5\n",
    "FAIL_PAST = True\n",
    "DIR_PATH = \"ST4000DM000_data\"\n",
    "OUT_PATH = \"data/\" + DIR_PATH + \"/\"\n",
    "directory = os.listdir(DIR_PATH)\n",
    "CONTIG_TRYS = 5;\n",
    "               \n",
    "SELECTED_COLS = [u'smart_1_raw', u'smart_4_raw',u'smart_5_raw', u'smart_7_raw', \n",
    "        u'smart_9_raw', u'smart_10_raw',  u'smart_12_raw', u'smart_183_raw', \n",
    "        u'smart_184_raw', u'smart_187_raw', u'smart_188_raw', \n",
    "        u'smart_189_raw', u'smart_190_raw', u'smart_192_raw', u'smart_193_raw', \n",
    "        u'smart_194_raw', u'smart_197_raw', u'smart_198_raw', u'smart_199_raw', \n",
    "        u'smart_240_raw', u'smart_241_raw',u'smart_242_raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_run(disk):\n",
    "    \"\"\"\n",
    "    Trys to get a contiguous run of days equal to DAY_BACK\n",
    "    A contiguous set of days may not exist\n",
    "    There could be too few days or the days may be not contiguous\n",
    "    If day back lands at a day which does not exist, this must be handeled\n",
    "    attempts to handle try_count times\n",
    "    \"\"\"\n",
    "    try_count = 0\n",
    "    while try_count < CONTIG_TRYS:\n",
    "        ran_range = int(len(disk)  - DAY_BUFFER_FACTOR*DAY_BACK)\n",
    "        if ran_range < 1:\n",
    "            continue\n",
    "        ran_num = np.random.randint(ran_range) +  DAY_BACK\n",
    "        lastdayfrom = disk.index[ran_num]\n",
    "        try:\n",
    "            disk_run = disk.loc[lastdayfrom - pd.Timedelta(days=DAY_BACK):lastdayfrom].reset_index()\n",
    "        except:\n",
    "            try_count +=1\n",
    "            continue\n",
    "        if len(disk_run) == DAY_BACK + 1:\n",
    "            return disk_run\n",
    "        else:\n",
    "            try_count +=1\n",
    "    return []\n",
    "\n",
    "def get_failure_run(disk):\n",
    "    \"\"\"\n",
    "    Trys to get a contiguous run of days equal to DAY_BACK for a disk that failed\n",
    "    because few failures exist, do a lot to avoid these\n",
    "    \"\"\"\n",
    "    disk_len = len(disk)\n",
    "    if disk_len >= DAY_BACK:\n",
    "        max_day_back = DAY_BACK\n",
    "    else:\n",
    "        max_day_back = disk_len\n",
    "    disk = disk.set_index('date')\n",
    "    try_count = 0\n",
    "    avail_day_back = max_day_back\n",
    "    while try_count < CONTIG_TRYS:\n",
    "        lastdayfrom = disk.index[len(disk)-1]\n",
    "        try:\n",
    "            disk_run = disk.loc[lastdayfrom - pd.Timedelta(days=avail_day_back):lastdayfrom].reset_index()\n",
    "            if len(disk_run) < max_day_back + 1 and avail_day_back < disk_len:\n",
    "                #print \"days were not contiguous, but there are more days left in array\", len(disk_run), avail_day_back, disk_len\n",
    "                try_count += 1\n",
    "                avail_day_back += 1\n",
    "                continue\n",
    "            else:\n",
    "                #print \"days were not contiguous and there are no more days in array\", try_count\n",
    "                return disk_run, _\n",
    "        except:\n",
    "            #print \"landed on a day that did not exist\", avail_day_back, disk_len\n",
    "            try_count +=1\n",
    "            if avail_day_back < disk_len:\n",
    "                avail_day_back += 1\n",
    "                continue\n",
    "            return [], \"D\"\n",
    "    return [], \"M\"\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk:  2000\n",
      "n_lost_nominals 1\n",
      "n_lost_failures 0\n",
      "incomplete failures:  1\n",
      "disctoninous failures:  0\n",
      "total number of failures resolved:  101\n",
      "disk:  4000\n",
      "n_lost_nominals 1\n",
      "n_lost_failures 0\n",
      "incomplete failures:  2\n",
      "disctoninous failures:  0\n",
      "total number of failures resolved:  195\n",
      "disk:  6000\n",
      "n_lost_nominals 2\n",
      "n_lost_failures 0\n",
      "incomplete failures:  3\n",
      "disctoninous failures:  0\n",
      "total number of failures resolved:  280\n",
      "disk:  8000\n",
      "n_lost_nominals 4\n",
      "n_lost_failures 0\n",
      "incomplete failures:  4\n",
      "disctoninous failures:  0\n",
      "total number of failures resolved:  368\n",
      "disk:  10000\n",
      "n_lost_nominals 6\n",
      "n_lost_failures 0\n",
      "incomplete failures:  4\n",
      "disctoninous failures:  0\n",
      "total number of failures resolved:  442\n",
      "disk:  12000\n",
      "n_lost_nominals 6\n",
      "n_lost_failures 0\n",
      "incomplete failures:  4\n",
      "disctoninous failures:  0\n",
      "total number of failures resolved:  523\n",
      "disk:  14000\n",
      "n_lost_nominals 6\n",
      "n_lost_failures 0\n",
      "incomplete failures:  4\n",
      "disctoninous failures:  2\n",
      "total number of failures resolved:  614\n",
      "disk:  16000\n",
      "n_lost_nominals 9\n",
      "n_lost_failures 0\n",
      "incomplete failures:  6\n",
      "disctoninous failures:  2\n",
      "total number of failures resolved:  698\n",
      "disk:  18000\n",
      "n_lost_nominals 11\n",
      "n_lost_failures 0\n",
      "incomplete failures:  9\n",
      "disctoninous failures:  2\n",
      "total number of failures resolved:  769\n",
      "disk:  20000\n",
      "n_lost_nominals 13\n",
      "n_lost_failures 0\n",
      "incomplete failures:  12\n",
      "disctoninous failures:  3\n",
      "total number of failures resolved:  865\n",
      "disk:  22000\n",
      "n_lost_nominals 15\n",
      "n_lost_failures 0\n",
      "incomplete failures:  17\n",
      "disctoninous failures:  3\n",
      "total number of failures resolved:  941\n",
      "disk:  24000\n",
      "n_lost_nominals 15\n",
      "n_lost_failures 0\n",
      "incomplete failures:  18\n",
      "disctoninous failures:  3\n",
      "total number of failures resolved:  1014\n",
      "disk:  26000\n",
      "n_lost_nominals 18\n",
      "n_lost_failures 0\n",
      "incomplete failures:  19\n",
      "disctoninous failures:  3\n",
      "total number of failures resolved:  1105\n",
      "disk:  28000\n",
      "n_lost_nominals 19\n",
      "n_lost_failures 0\n",
      "incomplete failures:  23\n",
      "disctoninous failures:  3\n",
      "total number of failures resolved:  1191\n",
      "disk:  30000\n",
      "n_lost_nominals 19\n",
      "n_lost_failures 0\n",
      "incomplete failures:  23\n",
      "disctoninous failures:  3\n",
      "total number of failures resolved:  1273\n",
      "disk:  32000\n",
      "n_lost_nominals 19\n",
      "n_lost_failures 0\n",
      "incomplete failures:  29\n",
      "disctoninous failures:  4\n",
      "total number of failures resolved:  1347\n",
      "disk:  34000\n",
      "n_lost_nominals 19\n",
      "n_lost_failures 0\n",
      "incomplete failures:  30\n",
      "disctoninous failures:  4\n",
      "total number of failures resolved:  1429\n",
      "disk:  36000\n",
      "n_lost_nominals 19\n",
      "n_lost_failures 0\n",
      "incomplete failures:  31\n",
      "disctoninous failures:  4\n",
      "total number of failures resolved:  1514\n"
     ]
    }
   ],
   "source": [
    "col_len = len(ORIG_COLS)\n",
    "n_lost_failures = 0\n",
    "n_lost_nominals = 0\n",
    "n_incomplete_failures = 0\n",
    "n_discontig_failures = 0\n",
    "disk_bin = []\n",
    "y = []\n",
    "z = []\n",
    "n = 0\n",
    "for disk_file in directory:\n",
    "    \n",
    "    if disk_file.split('.')[1] != 'csv':\n",
    "        continue\n",
    "        \n",
    "    n += 1\n",
    "    if TEST and n > 2001:\n",
    "        break\n",
    "        \n",
    "    if n % 2000 == 0 :\n",
    "        print 'disk: ', n\n",
    "        print 'n_lost_nominals', n_lost_nominals\n",
    "        print 'n_lost_failures', n_lost_failures \n",
    "        print 'incomplete failures: ', n_incomplete_failures \n",
    "        print 'disctoninous failures: ', n_discontig_failures\n",
    "        print 'total number of failures resolved: ', np.sum(y)\n",
    "    \n",
    "    disk = pd.read_csv(DIR_PATH + '/' + disk_file, comment=\"#\", header=0)\n",
    "    if len(disk.columns) != col_len:\n",
    "        print \"Oh no! \", disk_file , \" has too few/many columns!\"\n",
    "        continue\n",
    "    else:\n",
    "        for col in disk.columns:\n",
    "            if col not in ORIG_COLS:\n",
    "                print \"Oh no! \", disk_file , \" has strange col: \", col\n",
    "        \n",
    "    disk['date'] = pd.to_datetime(disk['date'], format='%Y-%m-%d')        \n",
    "    fail_day = disk['date'][disk['failure']>=1].values\n",
    "    if len(fail_day) > 1:\n",
    "        print \"Oh no! \", disk_file , \" is a zombie hard drive returning from dead!\"\n",
    "        continue\n",
    "        \n",
    "    if len(fail_day) == 0:\n",
    "        #fake_day = DISTANT_DATE\n",
    "        disk['day2fail'] = DISTANT_DATE - disk['date']\n",
    "        if len(disk) > DAY_BUFFER_FACTOR * DAY_BACK:\n",
    "            disk = disk.set_index('date')\n",
    "            disk_run = get_run(disk) \n",
    "            if any(disk_run):\n",
    "                y.append(0)\n",
    "                z.append(disk_run['day2fail'].values)\n",
    "                #for col in disk_run.columns:\n",
    "                #    if col not in SELECTED_COLS:\n",
    "                #        del disk_run[col]\n",
    "                disk_bin.append(disk_run)\n",
    "            else:\n",
    "                n_lost_nominals +=1\n",
    "            \n",
    "    if len(fail_day) == 1:\n",
    "        disk['day2fail'] = fail_day[0] - disk['date']\n",
    "        if FAIL_PAST == 1:\n",
    "            disk['failure'] = 1\n",
    "        disk_run, s = get_failure_run(disk)\n",
    "        if s == \"D\":\n",
    "            n_discontig_failures += 1\n",
    "            continue\n",
    "        if s == \"M\":\n",
    "            n_incomplete_failures += 1\n",
    "            continue\n",
    "        if any(disk_run):\n",
    "            y.append(1)\n",
    "            z.append(disk_run['day2fail'].values)\n",
    "            #for col in disk_run.columns:\n",
    "            #    if col not in SELECTED_COLS:\n",
    "            #        del disk_run[col]\n",
    "            disk_bin.append(disk_run)\n",
    "        else:\n",
    "            n_lost_failures +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_lost_nominals 19\n",
      "n_lost_failures 0\n",
      "incomplete failures:  31\n",
      "disctoninous failures:  4\n",
      "total number of failures resolved:  1532\n",
      "disk bin length:  36066\n"
     ]
    }
   ],
   "source": [
    "print 'n_lost_nominals', n_lost_nominals\n",
    "print 'n_lost_failures', n_lost_failures \n",
    "print 'incomplete failures: ', n_incomplete_failures \n",
    "print 'disctoninous failures: ', n_discontig_failures\n",
    "print 'total number of failures resolved: ', np.sum(y)\n",
    "print 'disk bin length: ', len(disk_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = open(OUT_PATH + 'disk_bin_d' + str(DAY_BACK) + '.pkl', 'wb')\n",
    "pickle.dump(disk_bin, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining dividual drives behavior is interesting. For example on the first data munge found:\n",
    " * Z300XJJ2.csv, Z300XH5X.csv had no values just commas for first 30 lines or so\n",
    "\n",
    " * Z30149QL.csv, has skips in data, no data for several days after 2014-07-20, then resumes normally\n",
    "\n",
    " * Z3025KZV.csv has a gap from 2014 to 2015 and smart9 resets\n",
    "\n",
    " * S300XCP4.csv had dates that didn't match the smart9 drive hours\n",
    " \n",
    "Finally we make a non-ragged clean tensor from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'disk_bin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0771ad18605e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisk_bin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDAY_BACK\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'disk_bin' is not defined"
     ]
    }
   ],
   "source": [
    "x_ = []\n",
    "y_ = []\n",
    "z_ = []\n",
    "\n",
    "n = 0\n",
    "for item in disk_bin:\n",
    "    tmp = z[n]/np.timedelta64(1, 'D')\n",
    "    if len(tmp) == DAY_BACK+1:\n",
    "        for col in item.columns:\n",
    "            if col not in SELECTED_COLS:\n",
    "                del item[col]\n",
    "        x_.append(item.values)\n",
    "        z_.append(tmp)\n",
    "        y_.append(y[n])\n",
    "    n += 1\n",
    "    \n",
    "print np.shape(x_)\n",
    "print np.shape(z_)\n",
    "print np.shape(y_)\n",
    "print np.sum(y_)\n",
    "\n",
    "np.save(OUT_PATH + 'train_d' + str(DAY_BACK) + '.npy', x_)\n",
    "np.save(OUT_PATH + 'label_d' + str(DAY_BACK) + '.npy', y_)\n",
    "np.save(OUT_PATH + 'lookback_d'+ str(DAY_BACK) + '.npy', z_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
