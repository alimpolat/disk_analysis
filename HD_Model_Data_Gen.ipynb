{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Drive Model Data Generator\n",
    "* pulls out many features over all times for all drives of a given MODEL\n",
    "* prepares data for time series machine learning\n",
    "* writes a .csv (into OUTPUT_DIR) for each unqiue 'serial_number' of a given MODEL\n",
    "* the output data features include all columns that contain the string FEATURE_COL\n",
    "* generates many files! a file for each unique serial # of a given model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import gmtime, strftime, time\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODEL = 'ST4000DM000'\n",
    "TEST = False\n",
    "DATA_DIR = \"data/\"\n",
    "DATA_FOLDERS = [\"2014\",\"2015\",\"data_Q1_2016\",\"data_Q2_2016\",\"data_Q3_2016\",\"data_Q4_2016\"]\n",
    "NAN_FRAC = 12 \n",
    "FEATURE_COL = \"raw\"\n",
    "COLS = [u'date', u'failure', u'serial_number', u'smart_1_raw', u'smart_3_raw', u'smart_4_raw',\n",
    "       u'smart_5_raw', u'smart_7_raw', u'smart_9_raw', u'smart_10_raw',\n",
    "       u'smart_12_raw', u'smart_183_raw', u'smart_184_raw', u'smart_187_raw',\n",
    "       u'smart_188_raw', u'smart_189_raw', u'smart_190_raw', u'smart_191_raw',\n",
    "       u'smart_192_raw', u'smart_193_raw', u'smart_194_raw', u'smart_197_raw',\n",
    "       u'smart_198_raw', u'smart_199_raw', u'smart_240_raw', u'smart_241_raw',\n",
    "       u'smart_242_raw']\n",
    "THE_TIME = strftime(\"%Y-%m-%d-%H-%S\", gmtime())\n",
    "OUTPUT_DIR = MODEL + \"_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets up a custom logger that will work with ipython, repeated calls to this piece of code without restarting the IPython kernel will result in creating and attaching yet another handler to the logger on every run and the number of messages being logged to the file with each log call will grow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def start_logger():\n",
    "    if os.path.isdir(OUTPUT_DIR):\n",
    "        os.system(\"rm \" + OUTPUT_DIR + \"/*\")\n",
    "        os.system(\"rmdir \" + OUTPUT_DIR)\n",
    "    os.system(\"mkdir \" + OUTPUT_DIR)\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.DEBUG)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logfile = OUTPUT_DIR + '/' + THE_TIME + \".log\"\n",
    "    handler = logging.FileHandler(logfile, 'w')\n",
    "    handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "    logger.info(\"MODEL\\t=\\t\" + MODEL)\n",
    "    logger.info(\"TEST\\t=\\t\" + str(TEST))\n",
    "    logger.info(\"DATA_DIR\\t=\\t\" + DATA_DIR)\n",
    "    logger.info(\"OUTPUT_DIR\\t=\\t\" + OUTPUT_DIR)\n",
    "    logger.info(\"THE_TIME\\t=\\t\" + THE_TIME)\n",
    "    logger.info(\"COLS\\t=\\t\" + str(COLS))\n",
    "    logger.info(\"NAN_FRAC\\t=\\t\" + str(NAN_FRAC))\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select specific MODEL and drop columns that do not contain FEATURE_COL, are date or are serial\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sift_data_dep(data):\n",
    "    sifted_data = pd.DataFrame.copy(data[data['model'] == MODEL])\n",
    "    for ikey in sifted_data.keys():\n",
    "        if FEATURE_COL not in ikey and ikey != 'serial_number' and ikey != 'failure' and ikey != 'date':\n",
    "                del sifted_data[ikey]\n",
    "    return sifted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sift_data(data):\n",
    "    sifted_data = pd.DataFrame.copy(data[data['model'] == MODEL])\n",
    "    for ikey in sifted_data.keys():\n",
    "        if ikey not in COLS:\n",
    "                del sifted_data[ikey]\n",
    "    return sifted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop columns that are all NaN or have more than NAN_FRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " def clean_data_aggresive(data):\n",
    "    hd_clean = data.dropna(axis=1, how='all')\n",
    "    for ikey in hd_clean.keys():\n",
    "        column_nan_count = hd_clean[ikey].isnull().sum()\n",
    "        logger.info(\"%d NaN fraction of:\\t %s\" % (column_nan_count/len(hd_clean), ikey))\n",
    "        if column_nan_count / len(hd_clean) >= NAN_FRAC:\n",
    "            logger.info(\"Dropping column: %s\" % ikey)\n",
    "            hd_clean = hd_clean.drop(ikey, 1)\n",
    "\n",
    "    for ikey in hd_clean.keys():\n",
    "        column_unique_count = len(hd_clean[ikey].unique())\n",
    "        logging.info(\"%d unique value count of:\\t %s\" % (column_unique_count, ikey))\n",
    "        if column_unique_count == 1 and ikey != 'failure':\n",
    "                logging.info(\"Dropping column: %s\" % ikey)\n",
    "                hd_clean = hd_clean.drop(ikey, 1)\n",
    "\n",
    "    hd_clean = hd_clean.fillna(666)\n",
    "\n",
    "    for ikey in hd_clean.keys():\n",
    "        column_nan_count = hd_clean[ikey].isnull().sum()\n",
    "        if column_nan_count > 0:\n",
    "            logger.warning(\"Warning! There shouldn't be any NaNs but there are in %s\" % ikey)\n",
    "    return hd_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def clean_data(data):\n",
    "    \"\"\"\n",
    "    crtical clean question, how to deal with NaN?\n",
    "    \"\"\"\n",
    "    data = data.dropna(axis=1, how='all') # if all na, really bad, just drop\n",
    "    data['NaNs'] = data.isnull().sum(axis=1) # save the number of NaNs for later\n",
    "    data = data.drop(data[data.NaNs >= NAN_FRAC].index) # if more than NaN frac, just drop\n",
    "    hd_clean = data.fillna(0) # else fill with 666 or zero?\n",
    "    return hd_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = start_logger()\n",
    "start_time = time()\n",
    "\n",
    "\n",
    "for data_dir in DATA_FOLDERS:\n",
    "    data_dir = DATA_DIR + data_dir + \"/\"\n",
    "    hd = pd.DataFrame()\n",
    "    logger.info(\" ### ### ### ### ### ### ### ### ### ### #### ### ###\")\n",
    "    logger.info(\"Working with: %s\" % data_dir)\n",
    "    logger.info(\"Elapsed time: %s seconds\" % np.round(time() - start_time,1))\n",
    "    \n",
    "    for data_file in os.listdir(data_dir):\n",
    "        if data_file.split('.')[1] == 'csv':\n",
    "            if TEST:\n",
    "                temp = pd.read_csv(data_dir+data_file, header=0, nrows=200)\n",
    "                temp = sift_data(temp)\n",
    "            else:\n",
    "                temp = pd.read_csv(data_dir + data_file, header=0)\n",
    "                temp = sift_data(temp)\n",
    "            hd = hd.append(temp)\n",
    "\n",
    "    logger.info(\"Working on: %s\" % data_dir)\n",
    "    logger.info(\"Shape of this data: %s\" % str(np.shape(hd)))\n",
    "    logger.info(\"There are %d unique drives. \" % len(hd['serial_number'].unique()))    \n",
    "    logger.info(\"There are %d failures.\" % hd['failure'].sum())\n",
    "\n",
    "    clean = clean_data(hd)\n",
    "    hd = None\n",
    "    logger.info(\"There are %d unique drives after cleaning. \" % len(clean['serial_number'].unique()))\n",
    "    logger.info(\"There are %d failures after cleaning.\" % clean['failure'].sum())\n",
    "    \n",
    "    serial_grouped = clean.groupby(['serial_number'])\n",
    "    serial_grouped = list(serial_grouped)\n",
    "\n",
    "    logger.info(\"Writing serial csv files from %s\" % data_dir)\n",
    "\n",
    "    for i in range((len(serial_grouped))):\n",
    "        fname = OUTPUT_DIR + \"/\" + serial_grouped[i][0] + '.csv'\n",
    "        individual_serial_data = serial_grouped[i][1]\n",
    "        del individual_serial_data['serial_number']\n",
    "        individual_serial_data = individual_serial_data.sort_values(by=['smart_9_raw'])\n",
    "        individual_serial_data.to_csv(path_or_buf=fname, sep=',', header=True, index=False, mode='a',\n",
    "                                      encoding='utf-8')\n",
    "        f = open(fname, 'a')\n",
    "        f.write(\"#\")\n",
    "        f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
