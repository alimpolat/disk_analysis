{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Drive Model Data Generator\n",
    "* pulls out many features over all times for all drives of a given MODEL\n",
    "* prepares data for time series machine learning\n",
    "* writes a .csv (into OUTPUT_DIR) for each unqiue 'serial_number' of a given MODEL\n",
    "* the output data features include all columns that contain the string FEATURE_COL\n",
    "* note: generates many files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import gmtime, strftime\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODEL = 'ST4000DM000'\n",
    "TEST = False\n",
    "DATA_DIR = \"data/\"\n",
    "DATA_FOLDERS = [\"2014\",\"2015\",\"data_Q1_2016\",\"data_Q2_2016\",\"data_Q3_2016\",\"data_Q4_2016\"]\n",
    "NAN_FRAC = .25 \n",
    "FEATURE_COL = \"raw\"\n",
    "THE_TIME = strftime(\"%Y-%m-%d-%H-%S\", gmtime())\n",
    "OUTPUT_DIR = MODEL + \"_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets up a custom logger that will work with ipython, repeated calls to this piece of code without restarting the IPython kernel will result in creating and attaching yet another handler to the logger on every run and the number of messages being logged to the file with each log call will grow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def start_logger():\n",
    "    if os.path.isdir(OUTPUT_DIR):\n",
    "        os.system(\"rm \" + OUTPUT_DIR + \"/*\")\n",
    "        os.system(\"rmdir \" + OUTPUT_DIR)\n",
    "    os.system(\"mkdir \" + OUTPUT_DIR)\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.DEBUG)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logfile = OUTPUT_DIR + '/' + THE_TIME + \".log\"\n",
    "    handler = logging.FileHandler(logfile, 'w')\n",
    "    handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "    logger.info(\"MODEL\\t=\\t\" + MODEL)\n",
    "    logger.info(\"TEST\\t=\\t\" + str(TEST))\n",
    "    logger.info(\"DATA_DIR\\t=\\t\" + DATA_DIR)\n",
    "    logger.info(\"OUTPUT_DIR\\t=\\t\" + OUTPUT_DIR)\n",
    "    logger.info(\"THE_TIME\\t=\\t\" + THE_TIME)\n",
    "    logger.info(\"FEATURE_COL\\t=\\t\" + FEATURE_COL)\n",
    "    logger.info(\"NAN_FRAC\\t=\\t\" + str(NAN_FRAC))\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select specific MODEL and drop columns that do not contain FEATURE_COL, are date or are serial\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sift_data(data):\n",
    "    sifted_data = pd.DataFrame.copy(data[data['model'] == MODEL])\n",
    "    for ikey in sifted_data.keys():\n",
    "        if FEATURE_COL not in ikey and ikey != 'serial_number' and ikey != 'failure' and ikey != 'date':\n",
    "                del sifted_data[ikey]\n",
    "    return sifted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop columns that are all NaN or have more than NAN_FRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " def clean_data(data):\n",
    "    hd_clean = data.dropna(axis=1, how='all')\n",
    "    for ikey in hd_clean.keys():\n",
    "        column_nan_count = hd_clean[ikey].isnull().sum()\n",
    "        logger.info(\"%d NaN fraction of:\\t %s\" % (column_nan_count/len(hd_clean), ikey))\n",
    "        if column_nan_count / len(hd_clean) >= NAN_FRAC:\n",
    "            logger.info(\"Dropping column: %s\" % ikey)\n",
    "            hd_clean = hd_clean.drop(ikey, 1)#, inplace=True)\n",
    "\n",
    "    for ikey in hd_clean.keys():\n",
    "        column_unique_count = len(hd_clean[ikey].unique())\n",
    "        logging.info(\"%d unique value count of:\\t %s\" % (column_unique_count, ikey))\n",
    "        #if column_unique_count == 1 and ikey != 'failure':\n",
    "        #        logging.info(\"Dropping column: %s\" % ikey)\n",
    "        #        hd_clean = hd_clean.drop(ikey, 1)#, inplace=True)\n",
    "\n",
    "    hd_clean = hd_clean.fillna(666)\n",
    "\n",
    "    for ikey in hd_clean.keys():\n",
    "        column_nan_count = hd_clean[ikey].isnull().sum()\n",
    "        if column_nan_count > 0:\n",
    "            logger.warning(\"Warning! There shouldn't be any NaNs but there are in %s\" % ikey)\n",
    "    return hd_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:364 unique value count of:\t date\n",
      "INFO:root:12920 unique value count of:\t serial_number\n",
      "INFO:root:2 unique value count of:\t failure\n",
      "INFO:root:3186706 unique value count of:\t smart_1_raw\n",
      "INFO:root:2 unique value count of:\t smart_3_raw\n",
      "INFO:root:123 unique value count of:\t smart_4_raw\n",
      "INFO:root:557 unique value count of:\t smart_5_raw\n",
      "INFO:root:3111730 unique value count of:\t smart_7_raw\n",
      "INFO:root:13646 unique value count of:\t smart_9_raw\n",
      "INFO:root:2 unique value count of:\t smart_10_raw\n",
      "INFO:root:103 unique value count of:\t smart_12_raw\n",
      "INFO:root:429 unique value count of:\t smart_183_raw\n",
      "INFO:root:7 unique value count of:\t smart_184_raw\n",
      "INFO:root:463 unique value count of:\t smart_187_raw\n",
      "INFO:root:1731 unique value count of:\t smart_188_raw\n",
      "INFO:root:671 unique value count of:\t smart_189_raw\n",
      "INFO:root:29 unique value count of:\t smart_190_raw\n",
      "INFO:root:2 unique value count of:\t smart_191_raw\n",
      "INFO:root:73 unique value count of:\t smart_192_raw\n",
      "INFO:root:91616 unique value count of:\t smart_193_raw\n",
      "INFO:root:29 unique value count of:\t smart_194_raw\n",
      "INFO:root:399 unique value count of:\t smart_197_raw\n",
      "INFO:root:341 unique value count of:\t smart_198_raw\n",
      "INFO:root:1359 unique value count of:\t smart_199_raw\n",
      "INFO:root:2825161 unique value count of:\t smart_240_raw\n",
      "INFO:root:3115312 unique value count of:\t smart_241_raw\n",
      "INFO:root:3116080 unique value count of:\t smart_242_raw\n",
      "INFO:__main__:There are 12920 unique drives after cleaning. \n",
      "INFO:__main__:There are 238 failures after cleaning.\n",
      "INFO:__main__:Writing serial csv files from data/2014/\n",
      "INFO:__main__:* * * \n",
      "Loading: data/2015/\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-68c31c823ab8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msift_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Working on: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity)\u001b[0m\n\u001b[1;32m   4433\u001b[0m             \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4434\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[0;32m-> 4435\u001b[0;31m                       verify_integrity=verify_integrity)\n\u001b[0m\u001b[1;32m   4436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4437\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m   1449\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1451\u001b[0;31m                        copy=copy)\n\u001b[0m\u001b[1;32m   1452\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[1;32m   1509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m             \u001b[0mndims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mconsolidate\u001b[0;34m(self, inplace)\u001b[0m\n\u001b[1;32m   2817\u001b[0m         \"\"\"\n\u001b[1;32m   2818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2819\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2820\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2821\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2799\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2801\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2803\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   2788\u001b[0m         \"\"\"\n\u001b[1;32m   2789\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2790\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2801\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconsolidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0mbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3525\u001b[0m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3526\u001b[0;31m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3530\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3531\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3532\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3533\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   4521\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4522\u001b[0m         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n\u001b[0;32m-> 4523\u001b[0;31m                                       _can_consolidate=_can_consolidate)\n\u001b[0m\u001b[1;32m   4524\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[1;32m   4544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4545\u001b[0m         \u001b[0margsort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4546\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4547\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger = start_logger()\n",
    "\n",
    "for data_dir in DATA_FOLDERS:\n",
    "    data_dir = DATA_DIR + data_dir + \"/\"\n",
    "    hd = pd.DataFrame()\n",
    "    logger.info(\"* * * \\nLoading: %s\" % data_dir)\n",
    "    for data_file in os.listdir(data_dir):\n",
    "        if data_file.split('.')[1] == 'csv':\n",
    "            if TEST:\n",
    "                temp = pd.read_csv(data_dir+data_file, header=0, nrows=200)\n",
    "                temp = sift_data(temp)\n",
    "            else:\n",
    "                temp = pd.read_csv(data_dir + data_file, header=0)\n",
    "                temp = sift_data(temp)\n",
    "            hd = hd.append(temp)\n",
    "\n",
    "    logger.info(\"Working on: %s\" % data_dir)\n",
    "    logger.info(\"Shape of this data: %s\" % str(np.shape(hd)))\n",
    "    logger.info(\"There are %d unique drives. \" % len(hd['serial_number'].unique()))    \n",
    "    logger.info(\"There are %d failures.\" % hd['failure'].sum())\n",
    "\n",
    "    clean = clean_data(hd)\n",
    "    \n",
    "    logger.info(\"There are %d unique drives after cleaning. \" % len(clean['serial_number'].unique()))\n",
    "    logger.info(\"There are %d failures after cleaning.\" % clean['failure'].sum())\n",
    "    \n",
    "    serial_grouped = clean.groupby(['serial_number'])\n",
    "    serial_grouped = list(serial_grouped)\n",
    "\n",
    "    logger.info(\"Writing serial csv files from %s\" % data_dir)\n",
    "\n",
    "    for i in range((len(serial_grouped))):\n",
    "        fname = OUTPUT_DIR + \"/\" + serial_grouped[i][0] + '.csv'\n",
    "        individual_serial_data = serial_grouped[i][1]\n",
    "        if os.path.exists(fname):\n",
    "            f = open(fname, 'a')\n",
    "        else:\n",
    "            f = open(fname, 'w')\n",
    "        f.write(\"#\" + str(np.shape(individual_serial_data)[0]) + \",\")\n",
    "        f.write(str(np.shape(individual_serial_data)[1]) + \",\")\n",
    "        f.write(data_dir + \"\\n#\")\n",
    "        f.close()\n",
    "        del individual_serial_data['serial_number']\n",
    "        individual_serial_data = individual_serial_data.sort_values(by=['smart_9_raw'])\n",
    "        individual_serial_data.to_csv(path_or_buf=fname, sep=',', header=True, index=False, mode='a',\n",
    "                                      encoding='utf-8')\n",
    "        #if TEST:\n",
    "        #    if i >= 10:\n",
    "        #        break\n",
    "\n",
    "\n",
    "#log_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
